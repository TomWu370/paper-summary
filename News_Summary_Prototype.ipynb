{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## Dissertation Project - News Summary Dissertation [100 marks]\n",
    "\n",
    "### Motivation \n",
    "\n",
    "> 1. Provide tools for anyone needing to speed up their research process\n",
    "> 2. Providing ways for user to quickly determine whether a piece of research is beneficial for their specific search terms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import HTML\n",
    "\n",
    "# HTML('''<script>\n",
    "# code_show=true; \n",
    "# function code_toggle() {\n",
    "#  if (code_show){\n",
    "#  $('div.input').hide();\n",
    "#  } else {\n",
    "#  $('div.input').show();\n",
    "#  }\n",
    "#  code_show = !code_show\n",
    "# } \n",
    "# $( document ).ready(code_toggle);\n",
    "# </script>\n",
    "# <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle code\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomwu\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "import fitz\n",
    "import fasttext\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import _pickle as pickle\n",
    "\n",
    "import transformers\n",
    "from simplet5 import SimpleT5\n",
    "from transformers import AutoTokenizer,BertTokenizer,T5Tokenizer,T5Config\n",
    "\n",
    "DATASET = \"./Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\tomwu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\tomwu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect which device (CPU/GPU) to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) \n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Summarisation model\n",
    "> 1. Dataset preprocessing\n",
    "> 2. Dataloader\n",
    "> 3. RNN model definition\n",
    "> 4. Model training\n",
    "> 5. Model prediction evaluation\n",
    "> 6. Dataset Exploration\n",
    "> 7. Dataset modification/Data Augmentation\n",
    "> 8. Model improvement\n",
    "> 9. Model finalisation and evaluation\n",
    "\n",
    "Paper querying\n",
    "> 1. Attention on query (Return usefulness percentage\n",
    "> 2. Evaluate performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comment this block if dataset is reorganised\n",
    "# DATA_DIR = \"SSN/papers.SSN.jsonl\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# with open(dataset_path) as f:\n",
    "#     lines = f.read().splitlines()\n",
    "# df_inter = pd.DataFrame(lines)\n",
    "# df_inter.columns = ['json_element']\n",
    "# df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "# df_final.to_json(\"./Dataset/SSN/SSN_Dataset.json\")\n",
    "# df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,summary in df_final.iterrows():\n",
    "#     temp = summary[\"abstract\"]\n",
    "#     print(type(summary[\"abstract\"]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Comment this block if dataset is shortened\n",
    "# DATA_DIR = \"SSN/SSN_Dataset.json\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# df = pd.read_json(dataset_path)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[140794][\"section_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return index for conclusion section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trim_index(row):\n",
    "#     return [row.index(x)+1 for x in row if x.startswith('conclusion') or x.startswith(\"summar\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trim_text(text, index):\n",
    "#     return text[0:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comment this block if dataset is shortened\n",
    "# # Trim text after conclusion\n",
    "# indexes = []\n",
    "# for i, row in df.iterrows():\n",
    "#     section = row[\"section_names\"]\n",
    "#     #print(section)\n",
    "#     index = trim_index(section)\n",
    "#     #print(index)\n",
    "#     if not index:\n",
    "#         indexes.append(i)\n",
    "#     # if section can be filtered\n",
    "#     else:\n",
    "#         index = index[0]\n",
    "#         abstract = row[\"abstract\"]\n",
    "#         text = row[\"text\"]\n",
    "#         section = row[\"section_names\"]\n",
    "#         df.at[i, \"section_names\"] = trim_text(section, index)\n",
    "#         df.at[i, \"abstract\"] = trim_text(abstract, index)\n",
    "#         df.at[i, \"text\"] = trim_text(text, index)\n",
    "# # dropping rows in dataframe that can't easily filter out reference section\n",
    "# print(len(indexes))\n",
    "# df.drop(indexes, inplace=True)\n",
    "# df.to_json(\"./Dataset/SSN/SSN_Dataset_Short.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35% of paper will be removed from the dataset due to it not having conclusion(s) and summary(ies) in their section titles, making it difficult to filter out the reference and appendix text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = \"SSN/SSN_Dataset_Short.json\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# df = pd.read_json(dataset_path)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See diversity in papers, select only computer science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_points = df['domain'].value_counts()\n",
    "# plot_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[df['domain'].isin([['Computer science']])]\n",
    "# df.to_json(\"./Dataset/SSN/SSN_Dataset_CompSci_Short.json\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"cnn_dailymail/train.csv\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# df = pd.read_csv(dataset_path)\n",
    "# df = df.rename(columns={'article': 'text', 'highlights': 'abstract'})\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"SSN/SSN_Dataset_CompSci_Short.json\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# df = pd.read_json(dataset_path)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df = df[[\"abstract\", \"text\"]]\n",
    "# summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any columns contain empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\'hi\\' \\ is used for space <br>\n",
    "random space, which is used for reference [15] quote box <br>\n",
    "sec ref is hyperlink to a section <br>\n",
    "fig ref is hyperlink to a figure <br>\n",
    "inlineform <br>\n",
    "displayform are both symbols, both contains numbers in string <br>\n",
    "remove all forms and remove all symbols but keep numbers <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_let(string):\n",
    "    return any(char.isalpha() for char in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_num(string):\n",
    "    return any(char.isdigit() for char in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_special(string, allowed):\n",
    "    '''\n",
    "    allowed is a list containing allowed symbols to pass detection\n",
    "    '''\n",
    "    return any(not(char.isalpha() or char.isdigit()) and (char not in allowed) for char in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLine(line, text=True, aug=False):\n",
    "    \"text parameter is to indicate whether the line is from text or abstract\"\n",
    "    symbols = [\"'\", \"’\"]\n",
    "    stop_words = list(ENGLISH_STOP_WORDS)\n",
    "    \n",
    "    clean_line = line.lower()\n",
    "\n",
    "    # fix apostrophes in line by removing apostrophe with no following alphabet character\n",
    "    clean_line = clean_line.replace(\"'\", \" \")\n",
    "    clean_line = clean_line.replace(\",\", \" \")\n",
    "#     # remove apostrophe if last character is apostrophe\n",
    "#     if clean_line and (clean_line[-1] == \"'\"):\n",
    "#         clean_line = clean_line[0:len(clean_line)-1]\n",
    "#     # fix apostrophes in line by removing space before single quote\n",
    "#     clean_line = clean_line.replace(\" '\", \"'\")\n",
    "\n",
    "\n",
    "    # clean line = clean line remove forms\n",
    "    words = clean_line.split()\n",
    "    #  remove forms, words with special characters inside\n",
    "    # if contain letter and number\n",
    "    # if contain special character not in allowed symbols and removing punctuations\n",
    "    # then remove\n",
    "    words = [x.replace(x, \"\") if (contain_let(x) and contain_num(x))\n",
    "             or contain_special(x, symbols)\n",
    "             else x for x in words]\n",
    "\n",
    "    # remove empty strings\n",
    "    words = filter(None, words)\n",
    "\n",
    "    # stop words from sklearn, remove stop words\n",
    "    if text:\n",
    "        words = [x for x in words if not x in stop_words]\n",
    "    # remove from line randomly\n",
    "    if text and aug:\n",
    "        choices = random.choices(words, k=math.floor(len(line)*0.2))\n",
    "        words.remove(choices)\n",
    "        \n",
    "     # combine the items into 1 string\n",
    "    clean_line = ' '.join(words)\n",
    "\n",
    "    \n",
    "\n",
    "    return clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concatParagraph(paragraph, text=True):\n",
    "#     clean_paragraph = \"\"\n",
    "#     for line in paragraph:\n",
    "#         lines = cleanLine(line)\n",
    "#         clean_paragraph += cleanLine(lines, text) + \" \"\n",
    "#         #print(clean_paragraph)\n",
    "        \n",
    "#     return(clean_paragraph.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concatPaper(paper, text=True):\n",
    "#     clean_paper = \"\"\n",
    "#     for paragraph in paper:\n",
    "#         clean_paper += concatParagraph(paragraph, text) + \" \"\n",
    "#     return(clean_paper.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# interval = 0.0001\n",
    "# aug_chance = 0.5\n",
    "# for i, row in tqdm(summary_df.iterrows(), total=summary_df.shape[0]):\n",
    "#     abstract = row[\"abstract\"]\n",
    "#     paper = row[\"text\"]\n",
    "    \n",
    "# #     summary_df.at[i, \"abstract\"] = concatParagraph(abstract, text=False)\n",
    "# #     summary_df.at[i, \"text\"] = concatPaper(paper)\n",
    "#     if random.random() > aug_chance:\n",
    "#         row_val = cleanLine(paper, text=True, aug=True)\n",
    "#         new_row = pd.Series([abstract, row_val], index=summary_df.columns)\n",
    "#         df_summary = df_summary.append(row1,ignore_index=True) \n",
    "#         df_summary\n",
    "#     summary_df.at[i, \"abstract\"] = cleanLine(abstract, text=True)\n",
    "#     summary_df.at[i, \"text\"] = cleanLine(paper)\n",
    "#     time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df.to_csv(\"./Dataset/cnn_dailymail/cleaned_cnn_train.csv\")\n",
    "# summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"cnn_dailymail/cleaned_cnn_train_150_short3.csv\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# df = pd.read_csv(dataset_path)\n",
    "# df = df[[\"source_text\", \"target_text\"]]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tree boosting highly effective widely used mac...</td>\n",
       "      <td>machine learning data driven approaches import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>face alignment task finding locations set faci...</td>\n",
       "      <td>face alignment refers finding pixel locations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>study pattern forming nonlinear dynamics start...</td>\n",
       "      <td>modulation instability mi fundamental process ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investigate infrared dynamics nonsupersymmetri...</td>\n",
       "      <td>understanding strong dynamics constitutes cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>propose new framework constructing polar codes...</td>\n",
       "      <td>polar codes family codes proven capacity achie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91381</th>\n",
       "      <td>propose dirichlet process mixtures generalized...</td>\n",
       "      <td>paper examine general regression problem gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91382</th>\n",
       "      <td>classify possible new u 1 x su 2 x su 3 multip...</td>\n",
       "      <td>higgs mass hierarchy puzzle suggests new physi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91383</th>\n",
       "      <td>second order linear ordinary diffrential equat...</td>\n",
       "      <td>motivation writing paper observation small app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91384</th>\n",
       "      <td>compare predictions spin independent contribut...</td>\n",
       "      <td>minimal supersymmetric standard model mssm bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91385</th>\n",
       "      <td>derive equations mean entropy mean internal en...</td>\n",
       "      <td>temperature stratified turbulence e g turbulen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91386 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "0      tree boosting highly effective widely used mac...   \n",
       "1      face alignment task finding locations set faci...   \n",
       "2      study pattern forming nonlinear dynamics start...   \n",
       "3      investigate infrared dynamics nonsupersymmetri...   \n",
       "4      propose new framework constructing polar codes...   \n",
       "...                                                  ...   \n",
       "91381  propose dirichlet process mixtures generalized...   \n",
       "91382  classify possible new u 1 x su 2 x su 3 multip...   \n",
       "91383  second order linear ordinary diffrential equat...   \n",
       "91384  compare predictions spin independent contribut...   \n",
       "91385  derive equations mean entropy mean internal en...   \n",
       "\n",
       "                                                    text  \n",
       "0      machine learning data driven approaches import...  \n",
       "1      face alignment refers finding pixel locations ...  \n",
       "2      modulation instability mi fundamental process ...  \n",
       "3      understanding strong dynamics constitutes cont...  \n",
       "4      polar codes family codes proven capacity achie...  \n",
       "...                                                  ...  \n",
       "91381  paper examine general regression problem gener...  \n",
       "91382  higgs mass hierarchy puzzle suggests new physi...  \n",
       "91383  motivation writing paper observation small app...  \n",
       "91384  minimal supersymmetric standard model mssm bes...  \n",
       "91385  temperature stratified turbulence e g turbulen...  \n",
       "\n",
       "[91386 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"SSN\\SSN_Dataset_Short_Clean.json\"\n",
    "dataset_path = DATASET+DATA_DIR\n",
    "df = pd.read_json(dataset_path)\n",
    "#df = df[[\"source_text\", \"target_text\"]]\n",
    "# df = df.rename(columns={'text': 'source_text', 'abstract': 'target_text'})\n",
    "#df['source_text'] = \"summarize: \" + df['source_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc26e7d8899403d873e87c70c194c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_max:  1000\n",
      "a_max:  100\n"
     ]
    }
   ],
   "source": [
    "t_max = 0\n",
    "a_max = 0\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    t_len = len(tokeniser(row['source_text'])['input_ids'])\n",
    "    a_len = len(tokeniser(row['target_text'])['input_ids'])\n",
    "    if t_len > t_max:\n",
    "        t_max = t_len\n",
    "    if a_len > a_max:\n",
    "        a_max = a_len\n",
    "print(\"t_max: \",t_max)\n",
    "print(\"a_max: \", a_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2163"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>source_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tracking developments highly dynamic data tech...</td>\n",
       "      <td>summarize: ubiquity online resources massive g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper propose new method enhance mapping paral...</td>\n",
       "      <td>summarize: large scale graph based application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>investigate models mitogenactivated protein ki...</td>\n",
       "      <td>summarize: mathematical modelling intra cellul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>machine learning used number security related ...</td>\n",
       "      <td>summarize: nowadays machine learning used numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>propose novel pose robust spatial aware gan ps...</td>\n",
       "      <td>summarize: work solve makeup transfer task tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>image restoration extensively researched topic...</td>\n",
       "      <td>summarize: image denoising problem researched ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>present neural encoder decoder model convert i...</td>\n",
       "      <td>summarize: optical character recognition ocr c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7984</th>\n",
       "      <td>ability detect pedestrians moving objects cruc...</td>\n",
       "      <td>summarize: autonomous cars currently primed ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7985</th>\n",
       "      <td>goal paper use multi task learning efficiently...</td>\n",
       "      <td>summarize: slot filling models useful method s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7986</th>\n",
       "      <td>propose novel deep supervised neural network t...</td>\n",
       "      <td>summarize: action recognition description vide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7987 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            target_text  \\\n",
       "0     tracking developments highly dynamic data tech...   \n",
       "1     paper propose new method enhance mapping paral...   \n",
       "2     investigate models mitogenactivated protein ki...   \n",
       "3     machine learning used number security related ...   \n",
       "4     propose novel pose robust spatial aware gan ps...   \n",
       "...                                                 ...   \n",
       "7982  image restoration extensively researched topic...   \n",
       "7983  present neural encoder decoder model convert i...   \n",
       "7984  ability detect pedestrians moving objects cruc...   \n",
       "7985  goal paper use multi task learning efficiently...   \n",
       "7986  propose novel deep supervised neural network t...   \n",
       "\n",
       "                                            source_text  \n",
       "0     summarize: ubiquity online resources massive g...  \n",
       "1     summarize: large scale graph based application...  \n",
       "2     summarize: mathematical modelling intra cellul...  \n",
       "3     summarize: nowadays machine learning used numb...  \n",
       "4     summarize: work solve makeup transfer task tra...  \n",
       "...                                                 ...  \n",
       "7982  summarize: image denoising problem researched ...  \n",
       "7983  summarize: optical character recognition ocr c...  \n",
       "7984  summarize: autonomous cars currently primed ma...  \n",
       "7985  summarize: slot filling models useful method s...  \n",
       "7986  summarize: action recognition description vide...  \n",
       "\n",
       "[7987 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(indexes, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_json(DATASET+\"SSN\\SSN_Dataset_CompSci_Short_Clean_HalfStop_170_Filter5.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSTRACT_MAX = 170\n",
    "# TEXT_MAX = 3050\n",
    "ABSTRACT_MAX = 200\n",
    "TEXT_MAX = 2800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac4b6f866db4941aac0e038aca90dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81119c048e4a43cda3686c2938ee05e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fa9cf8941f48a3a5ef04107618bedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955bf6b09383493ea6cab5cebb565a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained('google/t5-efficient-small') \n",
    "# takes into account of apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "6418 tracking\n",
      "11336 developments\n",
      "1385 highly\n",
      "4896 dynamic\n",
      "331 data\n",
      "748 technology\n",
      "3283 landscape\n",
      "3362 vital\n",
      "2627 keeping\n",
      "3714 novel\n",
      "2896 technologies\n",
      "1339 tools\n",
      "796 various\n",
      "844 areas\n",
      "7353 artificial\n",
      "6123 intelligence\n",
      "3 \n",
      "9 a\n",
      "23 i\n",
      "1256 difficult\n",
      "1463 track\n",
      "2193 relevant\n",
      "748 technology\n",
      "12545 keywords\n",
      "1040 paper\n",
      "4230 propose\n",
      "3714 novel\n",
      "7181 addresses\n",
      "682 problem\n",
      "1464 tool\n",
      "261 used\n",
      "3269 automatically\n",
      "8432 detect\n",
      "6831 existence\n",
      "126 new\n",
      "2896 technologies\n",
      "1339 tools\n",
      "1499 text\n",
      "5819 extract\n",
      "1353 terms\n",
      "261 used\n",
      "126 new\n",
      "2896 technologies\n",
      "21527 extracted\n",
      "126 new\n",
      "1353 terms\n",
      "3 \n",
      "13880 logged\n",
      "126 new\n",
      "3 \n",
      "9 a\n",
      "23 i\n",
      "2896 technologies\n",
      "3971 fly\n",
      "765 web\n",
      "3 \n",
      "14064 subsequently\n",
      "12910 classified\n",
      "2193 relevant\n",
      "27632 semantic\n",
      "11241 labels\n",
      "3 \n",
      "9 a\n",
      "23 i\n",
      "3303 domain\n",
      "7 s\n",
      "4382 proposed\n",
      "1464 tool\n",
      "3 \n",
      "390 based\n",
      "1726 stage\n",
      "1990 cas\n",
      "658 ca\n",
      "26 d\n",
      "53 ing\n",
      "825 model\n",
      "1726 stage\n",
      "853 class\n",
      "15821 ifies\n",
      "7142 sentence\n",
      "2579 contains\n",
      "748 technology\n",
      "1657 term\n",
      "511 second\n",
      "1726 stage\n",
      "3 \n",
      "8826 identifie\n",
      "7 s\n",
      "748 technology\n",
      "15693 keyword\n",
      "7142 sentence\n",
      "3442 obtain\n",
      "3265 competitive\n",
      "7452 accuracy\n",
      "4145 tasks\n",
      "7142 sentence\n",
      "13774 classification\n",
      "1499 text\n",
      "10356 identification\n",
      "1 </s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tracking developments highly dynamic data technology landscape vital keeping novel technologies tools various areas artificial intelligence ai difficult track relevant technology keywords paper propose novel addresses problem tool used automatically detect existence new technologies tools text extract terms used new technologies extracted new terms logged new ai technologies fly web subsequently classified relevant semantic labels ai domains proposed tool based stage cascading model stage classifies sentence contains technology term second stage identifies technology keyword sentence obtain competitive accuracy tasks sentence classification text identification</s>'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.at[0, \"target_text\"]\n",
    "print(len(tokeniser(text)[\"input_ids\"]))\n",
    "for i in tokeniser(text)[\"input_ids\"]:\n",
    "    print(i,tokeniser.decode(i))\n",
    "tokeniser.decode(tokeniser(text)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = []\n",
    "# t_max= 2524\n",
    "# a_max=  118\n",
    "# t_min=  668\n",
    "# a_min=  39\n",
    "# for i, row in df.iterrows():\n",
    "#     t_word = len(row[\"text\"].split())\n",
    "#     a_word = len(row[\"abstract\"].split())\n",
    "#     if ((t_word > t_max or t_word < t_min) or (a_word > a_max or a_word < a_min)):\n",
    "#         rows.append(i)\n",
    "\n",
    "# df.drop(rows, inplace=True)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_json(DATASET+\"SSN/SSN_Dataset_CompSci_Short_Clean_HalfStop_180.json\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DF to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_txt = df['text']\n",
    "# #text_txt = text_txt.str.removeprefix(\"summarize: \")\n",
    "# text_txt.to_csv(\"text_full_cnn.txt\", header=False,index=False)\n",
    "# # write to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_model = fasttext.train_unsupervised('text_full_cnn.txt', minn=2, epoch=10)\n",
    "# vocab_model.save_model(\"fastText_full_NoStop.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05246937, -0.06849538, -0.42895123,  0.25144276, -0.02151984,\n",
       "        0.05849433,  0.22607356, -0.2634456 , -0.04034835,  0.09165015,\n",
       "        0.23875414,  0.23043832,  0.335529  , -0.125991  , -0.23940863,\n",
       "       -0.31306407,  0.2726784 , -0.19945174,  0.09254331, -0.26865605,\n",
       "        0.1550692 ,  0.06794629, -0.12334667, -0.27257252, -0.63276577,\n",
       "       -0.00525512,  0.22396798,  0.46203488,  0.38846275,  0.4881754 ,\n",
       "       -0.33606574, -0.09112427,  0.25175312, -0.6451501 , -0.12379882,\n",
       "       -0.27533957, -0.48192084,  0.24728948,  0.35500985,  0.16617337,\n",
       "       -0.14461681, -0.455554  , -0.2539027 , -0.03580671,  0.27836385,\n",
       "       -0.20429875,  0.16872005, -0.24842629,  0.55424565,  0.37504348,\n",
       "       -0.12348666, -0.17431735, -0.02775216,  0.21767178, -0.32350695,\n",
       "        0.27022526,  0.169276  , -0.02366772,  0.0065518 , -0.39988786,\n",
       "        0.6083963 ,  0.13710539,  0.3647993 , -0.22767365,  0.21434541,\n",
       "       -0.06064311,  0.19077592, -0.6789286 ,  0.12426411, -0.13011889,\n",
       "        0.10088439,  0.07439702, -0.00915362,  0.14028543, -0.38548484,\n",
       "        0.14882556,  0.18410306,  0.10153061, -0.11437347,  0.3584751 ,\n",
       "        0.23404393,  0.3816993 ,  0.00503421, -0.17893104, -0.01761941,\n",
       "       -0.09162031,  0.27411357, -0.5735866 , -0.19327375, -0.12012877,\n",
       "        0.21460983, -0.38945127, -0.00236447, -0.08565319, -0.11523147,\n",
       "       -0.04293437,  0.07073183, -0.5786189 ,  0.7007944 ,  0.036686  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_model.get_word_vector('angle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9149702191352844, 'angles'),\n",
       " (0.7963276505470276, 'azimuthly'),\n",
       " (0.790732204914093, 'radian'),\n",
       " (0.7903612852096558, \"angle's\"),\n",
       " (0.7893282771110535, 'radians'),\n",
       " (0.7763140797615051, 'azimuth'),\n",
       " (0.7654001116752625, 'azimuthes'),\n",
       " (0.7620179653167725, 'azimuthial'),\n",
       " (0.738439679145813, 'plane'),\n",
       " (0.7361207604408264, 'altazimuth')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_model.get_nearest_neighbors('angle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, random_state=seed, train_size = 0.7)\n",
    "train_data, valid_data = train_test_split(train_data, random_state=seed, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleT5()\n",
    "model.load_model(model_type=\"t5\", model_dir=\"outputs/best\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 31.2 M\n",
      "-----------------------------------------------------\n",
      "31.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.2 M    Total params\n",
      "124.882   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomwu\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 42\n",
      "C:\\Users\\tomwu\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf8bff711ce4948a2923ed9811919d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train(train_df=train_data,\n",
    "        eval_df=valid_data,\n",
    "        source_max_token_len=TEXT_MAX,\n",
    "        target_max_token_len=ABSTRACT_MAX,\n",
    "        batch_size=2,\n",
    "        max_epochs=8,\n",
    "        use_gpu=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 32128\n",
      "d_model 512\n",
      "d_kv 64\n",
      "d_ff 2048\n",
      "num_layers 6\n",
      "num_decoder_layers 6\n",
      "num_heads 8\n",
      "relative_attention_num_buckets 32\n",
      "dropout_rate 0.1\n",
      "layer_norm_epsilon 1e-06\n",
      "initializer_factor 1.0\n",
      "feed_forward_proj relu\n",
      "use_cache True\n",
      "return_dict True\n",
      "output_hidden_states False\n",
      "output_attentions False\n",
      "torchscript False\n",
      "torch_dtype float32\n",
      "use_bfloat16 False\n",
      "pruned_heads {}\n",
      "tie_word_embeddings True\n",
      "is_encoder_decoder True\n",
      "is_decoder False\n",
      "cross_attention_hidden_size None\n",
      "add_cross_attention False\n",
      "tie_encoder_decoder False\n",
      "max_length 20\n",
      "min_length 0\n",
      "do_sample False\n",
      "early_stopping False\n",
      "num_beams 1\n",
      "num_beam_groups 1\n",
      "diversity_penalty 0.0\n",
      "temperature 1.0\n",
      "top_k 50\n",
      "top_p 1.0\n",
      "repetition_penalty 1.0\n",
      "length_penalty 1.0\n",
      "no_repeat_ngram_size 0\n",
      "encoder_no_repeat_ngram_size 0\n",
      "bad_words_ids None\n",
      "num_return_sequences 1\n",
      "chunk_size_feed_forward 0\n",
      "output_scores False\n",
      "return_dict_in_generate False\n",
      "forced_bos_token_id None\n",
      "forced_eos_token_id None\n",
      "remove_invalid_values False\n",
      "architectures ['T5ForConditionalGeneration']\n",
      "finetuning_task None\n",
      "id2label {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "label2id {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "tokenizer_class None\n",
      "prefix None\n",
      "bos_token_id None\n",
      "pad_token_id 0\n",
      "eos_token_id 1\n",
      "sep_token_id None\n",
      "decoder_start_token_id 0\n",
      "task_specific_params {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}\n",
      "problem_type None\n",
      "_name_or_path outputs/simplet5-epoch-0-train-loss-2.7644-val-loss-2.4369\n",
      "transformers_version 4.16.2\n",
      "model_type t5\n",
      "n_positions 512\n",
      "output_past True\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"outputs/simplet5-epoch-0-train-loss-2.7644-val-loss-2.4369\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"do_sample\": true,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.16.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = model.model.config.to_dict()\n",
    "for k, v in model_config.items():\n",
    "    print(k, v)\n",
    "print(\"----------------\")\n",
    "# model_config = model.model.config.to_dict()\n",
    "# model_config[\"num_heads\"] = 8\n",
    "#model_config[\"dropout_rate\"] = 0.2\n",
    "# model_config[\"d_model\"] = 512\n",
    "model_config[\"temperature\"] = 1 #1 or >1 for more random\n",
    "model_config[\"do_sample\"] = True\n",
    "config = T5Config(**model_config)\n",
    "# model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "model.model.config = config\n",
    "model.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "online social networks represent main source communication information exchange today's life facilitate exquisitely news sharing knowledge elicitation forming groups interests researchers decades studied growth dynamics online social networks extensively questing clear understanding behavior humans online social networks helps directions like engineering better recommendation systems attracting new members social networks achieved desired growth example online social networks like myspace orkut friendster service today work present probabilistic theoretical model captures dynamics social decay inactivity members social network model proved interesting mathematical properties imply achieving model optimization reasonable performance means maximization problem approximated factor 1 1 minimization problem achieved polynomial time\n",
      "-----------------\n",
      "['paper present empirical analysis social decay dynamics closed stack exchange websites presented model capturing decay dynamics social networks model probabilistic model assumes leave social network members affected leave neighbors theorem backstrom et al studied real data sets open stack exchange websites']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,row in valid_data.iterrows():\n",
    "    inputs = row[\"source_text\"]\n",
    "    output = model.predict(inputs,length_penalty=10,num_beams=25, repetition_penalty=2.5)\n",
    "    print(row[\"target_text\"])\n",
    "    print(\"-----------------\")\n",
    "    print(output)\n",
    "    print(\"\")\n",
    "    break\n",
    "    \n",
    "# inputs = valid_data.at[0,\"source_text\"]\n",
    "# print(inputs)\n",
    "# print(\"\")\n",
    "# print(valid_data.at[0,\"target_text\"])\n",
    "# model.device = \"cpu\"\n",
    "# model.predict(inputs,max_length=100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
