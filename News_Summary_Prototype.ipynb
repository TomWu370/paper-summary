{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## Dissertation Project - News Summary Dissertation [100 marks]\n",
    "\n",
    "### Motivation \n",
    "\n",
    "> 1. Provide tools for anyone needing to speed up their research process\n",
    "> 2. Providing ways for user to quickly determine whether a piece of research is beneficial for their specific search terms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "import fitz\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import json\n",
    "\n",
    "DATASET = \"./Dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect which device (CPU/GPU) to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) \n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Summarisation model\n",
    "> 1. Dataset preprocessing\n",
    "> 2. Dataloader\n",
    "> 3. RNN model definition\n",
    "> 4. Model training\n",
    "> 5. Model prediction evaluation\n",
    "> 6. Dataset Exploration\n",
    "> 7. Dataset modification/Data Augmentation\n",
    "> 8. Model improvement\n",
    "> 9. Model finalisation and evaluation\n",
    "\n",
    "Paper querying\n",
    "> 1. Attention on query (Return usefulness percentage\n",
    "> 2. Evaluate performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comment this block if dataset is reorganised\n",
    "# DATA_DIR = \"SSN/papers.SSN.jsonl\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# with open(dataset_path) as f:\n",
    "#     lines = f.read().splitlines()\n",
    "# df_inter = pd.DataFrame(lines)\n",
    "# df_inter.columns = ['json_element']\n",
    "# df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "# df_final.to_json(\"./Dataset/SSN/SSN_Dataset.json\")\n",
    "# df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,summary in df_final.iterrows():\n",
    "#     temp = summary[\"abstract\"]\n",
    "#     print(type(summary[\"abstract\"]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>section_names</th>\n",
       "      <th>domain</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4650265</td>\n",
       "      <td>XGBoost: A Scalable Tree Boosting System</td>\n",
       "      <td>[tree boosting is a highly effective and widel...</td>\n",
       "      <td>[introduction, tree boosting in a nutshell, re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[machine learning and data - driven approache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9195903</td>\n",
       "      <td>Robust Face Alignment Using a Mixture of Invar...</td>\n",
       "      <td>[face alignment , which is the task of finding...</td>\n",
       "      <td>[introduction, previous work, our approach, su...</td>\n",
       "      <td>[Computer science]</td>\n",
       "      <td>[[face alignment refers to finding the pixel l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119332442</td>\n",
       "      <td>Modulation instability associated nonlinear dy...</td>\n",
       "      <td>[we study pattern - forming nonlinear dynamics...</td>\n",
       "      <td>[introduction, modulation instability of becs ...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[modulation instability ( mi ) is one of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13494452</td>\n",
       "      <td>Free evolution on algebras with two states</td>\n",
       "      <td>[the key result in the paper concerns two tran...</td>\n",
       "      <td>[introduction, preliminaries, polynomials and ...</td>\n",
       "      <td>[Mathematics]</td>\n",
       "      <td>[[in a series of papers belinschi and nica int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119269876</td>\n",
       "      <td>Light Dilaton at Fixed Points and Ultra Light ...</td>\n",
       "      <td>[we investigate the infrared dynamics of a non...</td>\n",
       "      <td>[introduction, a comment on the large nn and n...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[understanding strong dynamics constitutes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140794</th>\n",
       "      <td>12157610</td>\n",
       "      <td>Minimal and Maximal Operator Spaces and Operat...</td>\n",
       "      <td>[we examine k - minimal and k - maximal operat...</td>\n",
       "      <td>[introduction, quantum information theory prel...</td>\n",
       "      <td>[Mathematics]</td>\n",
       "      <td>[[a primary goal of this paper is to formally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140795</th>\n",
       "      <td>14690185</td>\n",
       "      <td>AN OBSTRUCTION TO ASYMPTOTIC SEMISTABILITY AND...</td>\n",
       "      <td>[in this paper , we consider an obstruction to...</td>\n",
       "      <td>[introduction, statement of results, an obstru...</td>\n",
       "      <td>[Mathematics]</td>\n",
       "      <td>[[for a polarized algebraic manifold inlinefor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140796</th>\n",
       "      <td>15296646</td>\n",
       "      <td>Direct Detection of Neutralino Dark Matter and...</td>\n",
       "      <td>[we compare predictions for the spin - indepen...</td>\n",
       "      <td>[introduction, msugra, more general models, co...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[the minimal supersymmetric standard model ( ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140797</th>\n",
       "      <td>119576690</td>\n",
       "      <td>PCA by Optimisation of Symmetric Functions has...</td>\n",
       "      <td>[principal component analysis ( pca ) finds th...</td>\n",
       "      <td>[introduction, pca by determinant optimisation...</td>\n",
       "      <td>[Mathematics]</td>\n",
       "      <td>[[let inlineform0 be a data matrix , with rows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140798</th>\n",
       "      <td>119301470</td>\n",
       "      <td>Turbulent fluxes of entropy and internal energ...</td>\n",
       "      <td>[we derive equations for the mean entropy and,...</td>\n",
       "      <td>[introduction, turbulent convective flux of me...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[temperature stratified turbulence ( e.g. , t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140799 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         paper_id                                              title  \\\n",
       "0         4650265           XGBoost: A Scalable Tree Boosting System   \n",
       "1         9195903  Robust Face Alignment Using a Mixture of Invar...   \n",
       "2       119332442  Modulation instability associated nonlinear dy...   \n",
       "3        13494452         Free evolution on algebras with two states   \n",
       "4       119269876  Light Dilaton at Fixed Points and Ultra Light ...   \n",
       "...           ...                                                ...   \n",
       "140794   12157610  Minimal and Maximal Operator Spaces and Operat...   \n",
       "140795   14690185  AN OBSTRUCTION TO ASYMPTOTIC SEMISTABILITY AND...   \n",
       "140796   15296646  Direct Detection of Neutralino Dark Matter and...   \n",
       "140797  119576690  PCA by Optimisation of Symmetric Functions has...   \n",
       "140798  119301470  Turbulent fluxes of entropy and internal energ...   \n",
       "\n",
       "                                                 abstract  \\\n",
       "0       [tree boosting is a highly effective and widel...   \n",
       "1       [face alignment , which is the task of finding...   \n",
       "2       [we study pattern - forming nonlinear dynamics...   \n",
       "3       [the key result in the paper concerns two tran...   \n",
       "4       [we investigate the infrared dynamics of a non...   \n",
       "...                                                   ...   \n",
       "140794  [we examine k - minimal and k - maximal operat...   \n",
       "140795  [in this paper , we consider an obstruction to...   \n",
       "140796  [we compare predictions for the spin - indepen...   \n",
       "140797  [principal component analysis ( pca ) finds th...   \n",
       "140798  [we derive equations for the mean entropy and,...   \n",
       "\n",
       "                                            section_names              domain  \\\n",
       "0       [introduction, tree boosting in a nutshell, re...                  []   \n",
       "1       [introduction, previous work, our approach, su...  [Computer science]   \n",
       "2       [introduction, modulation instability of becs ...           [Physics]   \n",
       "3       [introduction, preliminaries, polynomials and ...       [Mathematics]   \n",
       "4       [introduction, a comment on the large nn and n...           [Physics]   \n",
       "...                                                   ...                 ...   \n",
       "140794  [introduction, quantum information theory prel...       [Mathematics]   \n",
       "140795  [introduction, statement of results, an obstru...       [Mathematics]   \n",
       "140796  [introduction, msugra, more general models, co...           [Physics]   \n",
       "140797  [introduction, pca by determinant optimisation...       [Mathematics]   \n",
       "140798  [introduction, turbulent convective flux of me...           [Physics]   \n",
       "\n",
       "                                                     text  \n",
       "0       [[machine learning and data - driven approache...  \n",
       "1       [[face alignment refers to finding the pixel l...  \n",
       "2       [[modulation instability ( mi ) is one of the ...  \n",
       "3       [[in a series of papers belinschi and nica int...  \n",
       "4       [[understanding strong dynamics constitutes a ...  \n",
       "...                                                   ...  \n",
       "140794  [[a primary goal of this paper is to formally ...  \n",
       "140795  [[for a polarized algebraic manifold inlinefor...  \n",
       "140796  [[the minimal supersymmetric standard model ( ...  \n",
       "140797  [[let inlineform0 be a data matrix , with rows...  \n",
       "140798  [[temperature stratified turbulence ( e.g. , t...  \n",
       "\n",
       "[140799 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Comment this block if dataset is shortened\n",
    "# DATA_DIR = \"SSN/SSN_Dataset.json\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# df = pd.read_json(dataset_path)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction',\n",
       " 'quantum information theory preliminaries',\n",
       " 'kk-minimal and kk-maximal operator spaces',\n",
       " 'kk-super minimal and kk-super maximal operator systems',\n",
       " 'norms on operator systems',\n",
       " 'contractive maps as separability criteria']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[140794][\"section_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return index for conclusion section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trim_index(row):\n",
    "#     return [row.index(x)+1 for x in row if x.startswith('conclusion') or x.startswith(\"summar\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trim_text(text, index):\n",
    "#     return text[0:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49413\n"
     ]
    }
   ],
   "source": [
    "# # Comment this block if dataset is shortened\n",
    "# # Trim text after conclusion\n",
    "# indexes = []\n",
    "# for i, row in df.iterrows():\n",
    "#     section = row[\"section_names\"]\n",
    "#     #print(section)\n",
    "#     index = trim_index(section)\n",
    "#     #print(index)\n",
    "#     if not index:\n",
    "#         indexes.append(i)\n",
    "#     # if section can be filtered\n",
    "#     else:\n",
    "#         index = index[0]\n",
    "#         abstract = row[\"abstract\"]\n",
    "#         text = row[\"text\"]\n",
    "#         section = row[\"section_names\"]\n",
    "#         df.at[i, \"section_names\"] = trim_text(section, index)\n",
    "#         df.at[i, \"abstract\"] = trim_text(abstract, index)\n",
    "#         df.at[i, \"text\"] = trim_text(text, index)\n",
    "# # dropping rows in dataframe that can't easily filter out reference section\n",
    "# print(len(indexes))\n",
    "# df.drop(indexes, inplace=True)\n",
    "# df.to_json(\"./Dataset/SSN/SSN_Dataset_Short.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35% of paper will be removed from the dataset due to it not having conclusion(s) and summary(ies) in their section titles, making it difficult to filter out the reference and appendix text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>section_names</th>\n",
       "      <th>domain</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4650265</td>\n",
       "      <td>XGBoost: A Scalable Tree Boosting System</td>\n",
       "      <td>[tree boosting is a highly effective and widel...</td>\n",
       "      <td>[introduction, tree boosting in a nutshell, re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[machine learning and data - driven approache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9195903</td>\n",
       "      <td>Robust Face Alignment Using a Mixture of Invar...</td>\n",
       "      <td>[face alignment , which is the task of finding...</td>\n",
       "      <td>[introduction, previous work, our approach, su...</td>\n",
       "      <td>[Computer science]</td>\n",
       "      <td>[[face alignment refers to finding the pixel l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119332442</td>\n",
       "      <td>Modulation instability associated nonlinear dy...</td>\n",
       "      <td>[we study pattern - forming nonlinear dynamics...</td>\n",
       "      <td>[introduction, modulation instability of becs ...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[modulation instability ( mi ) is one of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119269876</td>\n",
       "      <td>Light Dilaton at Fixed Points and Ultra Light ...</td>\n",
       "      <td>[we investigate the infrared dynamics of a non...</td>\n",
       "      <td>[introduction, a comment on the large nn and n...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[understanding strong dynamics constitutes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59413881</td>\n",
       "      <td>Decoder-tailored Polar Code Design Using the G...</td>\n",
       "      <td>[we propose a new framework for constructing p...</td>\n",
       "      <td>[introduction, polar codes, polar code constru...</td>\n",
       "      <td>[Computer science, Mathematics]</td>\n",
       "      <td>[[polar codes   are the first family of codes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91381</th>\n",
       "      <td>1520660</td>\n",
       "      <td>Dirichlet Process Mixtures of Generalized Line...</td>\n",
       "      <td>[we propose dirichlet process mixtures of gene...</td>\n",
       "      <td>[introduction, related work, mathematical back...</td>\n",
       "      <td>[Mathematics, Computer science]</td>\n",
       "      <td>[[in this paper , we examine the general regre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91382</th>\n",
       "      <td>118501110</td>\n",
       "      <td>Minimal Matter at the Large Hadron Collider</td>\n",
       "      <td>[we classify all possible new u(1 ) x su(2 ), ...</td>\n",
       "      <td>[introduction, new matter and its production, ...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[the higgs mass hierarchy puzzle suggests new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91383</th>\n",
       "      <td>16180248</td>\n",
       "      <td>On simulations of the classical harmonic oscil...</td>\n",
       "      <td>[we show that any second order linear ordinary...</td>\n",
       "      <td>[introduction, simplest discretizations of the...</td>\n",
       "      <td>[Physics, Mathematics]</td>\n",
       "      <td>[[the motivation for writing this paper is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91384</th>\n",
       "      <td>15296646</td>\n",
       "      <td>Direct Detection of Neutralino Dark Matter and...</td>\n",
       "      <td>[we compare predictions for the spin - indepen...</td>\n",
       "      <td>[introduction, msugra, more general models, co...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[the minimal supersymmetric standard model ( ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91385</th>\n",
       "      <td>119301470</td>\n",
       "      <td>Turbulent fluxes of entropy and internal energ...</td>\n",
       "      <td>[we derive equations for the mean entropy and,...</td>\n",
       "      <td>[introduction, turbulent convective flux of me...</td>\n",
       "      <td>[Physics]</td>\n",
       "      <td>[[temperature stratified turbulence ( e.g. , t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91386 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        paper_id                                              title  \\\n",
       "0        4650265           XGBoost: A Scalable Tree Boosting System   \n",
       "1        9195903  Robust Face Alignment Using a Mixture of Invar...   \n",
       "2      119332442  Modulation instability associated nonlinear dy...   \n",
       "3      119269876  Light Dilaton at Fixed Points and Ultra Light ...   \n",
       "4       59413881  Decoder-tailored Polar Code Design Using the G...   \n",
       "...          ...                                                ...   \n",
       "91381    1520660  Dirichlet Process Mixtures of Generalized Line...   \n",
       "91382  118501110        Minimal Matter at the Large Hadron Collider   \n",
       "91383   16180248  On simulations of the classical harmonic oscil...   \n",
       "91384   15296646  Direct Detection of Neutralino Dark Matter and...   \n",
       "91385  119301470  Turbulent fluxes of entropy and internal energ...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      [tree boosting is a highly effective and widel...   \n",
       "1      [face alignment , which is the task of finding...   \n",
       "2      [we study pattern - forming nonlinear dynamics...   \n",
       "3      [we investigate the infrared dynamics of a non...   \n",
       "4      [we propose a new framework for constructing p...   \n",
       "...                                                  ...   \n",
       "91381  [we propose dirichlet process mixtures of gene...   \n",
       "91382  [we classify all possible new u(1 ) x su(2 ), ...   \n",
       "91383  [we show that any second order linear ordinary...   \n",
       "91384  [we compare predictions for the spin - indepen...   \n",
       "91385  [we derive equations for the mean entropy and,...   \n",
       "\n",
       "                                           section_names  \\\n",
       "0      [introduction, tree boosting in a nutshell, re...   \n",
       "1      [introduction, previous work, our approach, su...   \n",
       "2      [introduction, modulation instability of becs ...   \n",
       "3      [introduction, a comment on the large nn and n...   \n",
       "4      [introduction, polar codes, polar code constru...   \n",
       "...                                                  ...   \n",
       "91381  [introduction, related work, mathematical back...   \n",
       "91382  [introduction, new matter and its production, ...   \n",
       "91383  [introduction, simplest discretizations of the...   \n",
       "91384  [introduction, msugra, more general models, co...   \n",
       "91385  [introduction, turbulent convective flux of me...   \n",
       "\n",
       "                                domain  \\\n",
       "0                                   []   \n",
       "1                   [Computer science]   \n",
       "2                            [Physics]   \n",
       "3                            [Physics]   \n",
       "4      [Computer science, Mathematics]   \n",
       "...                                ...   \n",
       "91381  [Mathematics, Computer science]   \n",
       "91382                        [Physics]   \n",
       "91383           [Physics, Mathematics]   \n",
       "91384                        [Physics]   \n",
       "91385                        [Physics]   \n",
       "\n",
       "                                                    text  \n",
       "0      [[machine learning and data - driven approache...  \n",
       "1      [[face alignment refers to finding the pixel l...  \n",
       "2      [[modulation instability ( mi ) is one of the ...  \n",
       "3      [[understanding strong dynamics constitutes a ...  \n",
       "4      [[polar codes   are the first family of codes ...  \n",
       "...                                                  ...  \n",
       "91381  [[in this paper , we examine the general regre...  \n",
       "91382  [[the higgs mass hierarchy puzzle suggests new...  \n",
       "91383  [[the motivation for writing this paper is an ...  \n",
       "91384  [[the minimal supersymmetric standard model ( ...  \n",
       "91385  [[temperature stratified turbulence ( e.g. , t...  \n",
       "\n",
       "[91386 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA_DIR = \"SSN/SSN_Dataset_Short.json\"\n",
    "# dataset_path = DATASET+DATA_DIR\n",
    "# df = pd.read_json(dataset_path)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tree boosting is a highly effective and widel...</td>\n",
       "      <td>[[machine learning and data - driven approache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[face alignment , which is the task of finding...</td>\n",
       "      <td>[[face alignment refers to finding the pixel l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[we study pattern - forming nonlinear dynamics...</td>\n",
       "      <td>[[modulation instability ( mi ) is one of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[we investigate the infrared dynamics of a non...</td>\n",
       "      <td>[[understanding strong dynamics constitutes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[we propose a new framework for constructing p...</td>\n",
       "      <td>[[polar codes   are the first family of codes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91381</th>\n",
       "      <td>[we propose dirichlet process mixtures of gene...</td>\n",
       "      <td>[[in this paper , we examine the general regre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91382</th>\n",
       "      <td>[we classify all possible new u(1 ) x su(2 ), ...</td>\n",
       "      <td>[[the higgs mass hierarchy puzzle suggests new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91383</th>\n",
       "      <td>[we show that any second order linear ordinary...</td>\n",
       "      <td>[[the motivation for writing this paper is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91384</th>\n",
       "      <td>[we compare predictions for the spin - indepen...</td>\n",
       "      <td>[[the minimal supersymmetric standard model ( ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91385</th>\n",
       "      <td>[we derive equations for the mean entropy and,...</td>\n",
       "      <td>[[temperature stratified turbulence ( e.g. , t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91386 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "0      [tree boosting is a highly effective and widel...   \n",
       "1      [face alignment , which is the task of finding...   \n",
       "2      [we study pattern - forming nonlinear dynamics...   \n",
       "3      [we investigate the infrared dynamics of a non...   \n",
       "4      [we propose a new framework for constructing p...   \n",
       "...                                                  ...   \n",
       "91381  [we propose dirichlet process mixtures of gene...   \n",
       "91382  [we classify all possible new u(1 ) x su(2 ), ...   \n",
       "91383  [we show that any second order linear ordinary...   \n",
       "91384  [we compare predictions for the spin - indepen...   \n",
       "91385  [we derive equations for the mean entropy and,...   \n",
       "\n",
       "                                                    text  \n",
       "0      [[machine learning and data - driven approache...  \n",
       "1      [[face alignment refers to finding the pixel l...  \n",
       "2      [[modulation instability ( mi ) is one of the ...  \n",
       "3      [[understanding strong dynamics constitutes a ...  \n",
       "4      [[polar codes   are the first family of codes ...  \n",
       "...                                                  ...  \n",
       "91381  [[in this paper , we examine the general regre...  \n",
       "91382  [[the higgs mass hierarchy puzzle suggests new...  \n",
       "91383  [[the motivation for writing this paper is an ...  \n",
       "91384  [[the minimal supersymmetric standard model ( ...  \n",
       "91385  [[temperature stratified turbulence ( e.g. , t...  \n",
       "\n",
       "[91386 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary_df = df[[\"abstract\", \"text\"]]\n",
    "# summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any columns contain empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abstract    False\n",
       "text        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\'hi\\' \\ is used for space <br>\n",
    "random space, which is used for reference [15] quote box <br>\n",
    "sec ref is hyperlink to a section <br>\n",
    "fig ref is hyperlink to a figure <br>\n",
    "inlineform <br>\n",
    "displayform are both symbols, both contains numbers in string <br>\n",
    "remove all forms and remove all symbols but keep numbers <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_let(string):\n",
    "    return any(char.isalpha() for char in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_num(string):\n",
    "    return any(char.isdigit() for char in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLine(line, text=True):\n",
    "    \"Text parameter is to indicate whether the line is from text or abstract\"\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    number = list(map(str, range(10)))\n",
    "    symbols = [\"'\", \"’\"]\n",
    "    valid_char = alphabet + number + symbols\n",
    "    stop_words = list(ENGLISH_STOP_WORDS)\n",
    "    \n",
    "    clean_line = line.lower()\n",
    "    \n",
    "    # fix apostrophes in line by removing apostrophe with no following alphabet character\n",
    "    clean_line = clean_line.replace(\"' \", \" \")\n",
    "    if clean_line and (clean_line[-1] == \"'\"):\n",
    "        clean_line = clean_line[0:len(clean_line)-1]\n",
    "    # fix apostrophes in line by removing space before single quote\n",
    "    clean_line = clean_line.replace(\" '\", \"'\")\n",
    "    #remove punctuation\n",
    "    # replace all non alphabet character with space\n",
    "    difference = list(set(clean_line).symmetric_difference(valid_char))\n",
    "    # consider removing words if they contain weird characters, as punctuations are\n",
    "    # separated with a space\n",
    "    for dif in difference:\n",
    "        clean_line = clean_line.replace(dif, \" \")\n",
    "    \n",
    "    # clean line = clean line remove forms\n",
    "    words = clean_line.split()\n",
    "\n",
    "    #  remove forms\n",
    "    words = [x.replace(x, \"\") if contain_let(x) and contain_num(x) else x for x in words]\n",
    "    # remove empty strings\n",
    "    words = filter(None, words)\n",
    "\n",
    "    # stop words from sklearn, remove stop words\n",
    "    if text:\n",
    "        words = [x for x in words if not x in stop_words]\n",
    "\n",
    "    # combine the items into 1 string\n",
    "    clean_line = ' '.join(words)\n",
    "    return clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatParagraph(paragraph, text):\n",
    "    clean_paragraph = \"\"\n",
    "    for line in paragraph:\n",
    "        lines = cleanLine(line)\n",
    "        clean_paragraph += cleanLine(lines, text) + \" \"\n",
    "        #print(clean_paragraph)\n",
    "        \n",
    "    return(clean_paragraph.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatPaper(paper, text):\n",
    "    clean_paper = \"\"\n",
    "    for paragraph in paper:\n",
    "        clean_paper += concatParagraph(paragraph, text) + \" \"\n",
    "    return(clean_paper.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c747d1730049bb899ed98de284d97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, row in tqdm(summary_df.iterrows(), total=df.shape[0]):\n",
    "    abstract = row[\"abstract\"]\n",
    "    paper = row[\"text\"]\n",
    "    \n",
    "    summary_df.at[i, \"abstract\"] = concatParagraph(abstract, text=False)\n",
    "    summary_df.at[i, \"text\"] = concatPaper(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tree boosting is a highly effective and widely...</td>\n",
       "      <td>machine learning and data driven approaches ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>face alignment which is the task of finding th...</td>\n",
       "      <td>face alignment refers to finding the pixel loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we study pattern forming nonlinear dynamics st...</td>\n",
       "      <td>modulation instability mi is one of the most f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we investigate the infrared dynamics of a nons...</td>\n",
       "      <td>understanding strong dynamics constitutes a co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we propose a new framework for constructing po...</td>\n",
       "      <td>polar codes are the first family of codes prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91381</th>\n",
       "      <td>we propose dirichlet process mixtures of gener...</td>\n",
       "      <td>in this paper we examine the general regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91382</th>\n",
       "      <td>we classify all possible new u 1 x su 2 x su 3...</td>\n",
       "      <td>the higgs mass hierarchy puzzle suggests new p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91383</th>\n",
       "      <td>we show that any second order linear ordinary ...</td>\n",
       "      <td>the motivation for writing this paper is an ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91384</th>\n",
       "      <td>we compare predictions for the spin independen...</td>\n",
       "      <td>the minimal supersymmetric standard model mssm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91385</th>\n",
       "      <td>we derive equations for the mean entropy and t...</td>\n",
       "      <td>temperature stratified turbulence e g turbulen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91386 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "0      tree boosting is a highly effective and widely...   \n",
       "1      face alignment which is the task of finding th...   \n",
       "2      we study pattern forming nonlinear dynamics st...   \n",
       "3      we investigate the infrared dynamics of a nons...   \n",
       "4      we propose a new framework for constructing po...   \n",
       "...                                                  ...   \n",
       "91381  we propose dirichlet process mixtures of gener...   \n",
       "91382  we classify all possible new u 1 x su 2 x su 3...   \n",
       "91383  we show that any second order linear ordinary ...   \n",
       "91384  we compare predictions for the spin independen...   \n",
       "91385  we derive equations for the mean entropy and t...   \n",
       "\n",
       "                                                    text  \n",
       "0      machine learning and data driven approaches ar...  \n",
       "1      face alignment refers to finding the pixel loc...  \n",
       "2      modulation instability mi is one of the most f...  \n",
       "3      understanding strong dynamics constitutes a co...  \n",
       "4      polar codes are the first family of codes prov...  \n",
       "...                                                  ...  \n",
       "91381  in this paper we examine the general regressio...  \n",
       "91382  the higgs mass hierarchy puzzle suggests new p...  \n",
       "91383  the motivation for writing this paper is an ob...  \n",
       "91384  the minimal supersymmetric standard model mssm...  \n",
       "91385  temperature stratified turbulence e g turbulen...  \n",
       "\n",
       "[91386 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.to_json(\"./Dataset/SSN/SSN_Dataset_Short_Clean_NoStop.json\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we investigate the infrared dynamics of a nons...</td>\n",
       "      <td>understanding strong dynamics constitutes a co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this paper presents the first derivation of th...</td>\n",
       "      <td>cuscuton gravity was originally proposed about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in this brief review we examine the theoretica...</td>\n",
       "      <td>a recent milestone in observational cosmology ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we discuss leptogenesis within a tev scale inv...</td>\n",
       "      <td>one of the attractive features of the seesaw m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we address the question on how weak perturbati...</td>\n",
       "      <td>within the usual wisdom it is quite intuitive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9218</th>\n",
       "      <td>joint extraction of entities and relations is ...</td>\n",
       "      <td>joint extraction of entities and relations is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9219</th>\n",
       "      <td>robust classification algorithms have been dev...</td>\n",
       "      <td>a two sample test detects if two sets of data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9220</th>\n",
       "      <td>in a recent paper it was suggested a novel int...</td>\n",
       "      <td>deformations of special relativity dsr alleged...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>the ability to detect pedestrians and other mo...</td>\n",
       "      <td>autonomous cars are currently primed for mass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222</th>\n",
       "      <td>the goal of this paper is to use multi task le...</td>\n",
       "      <td>slot filling models are a useful method for si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9223 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     we investigate the infrared dynamics of a nons...   \n",
       "1     this paper presents the first derivation of th...   \n",
       "2     in this brief review we examine the theoretica...   \n",
       "3     we discuss leptogenesis within a tev scale inv...   \n",
       "4     we address the question on how weak perturbati...   \n",
       "...                                                 ...   \n",
       "9218  joint extraction of entities and relations is ...   \n",
       "9219  robust classification algorithms have been dev...   \n",
       "9220  in a recent paper it was suggested a novel int...   \n",
       "9221  the ability to detect pedestrians and other mo...   \n",
       "9222  the goal of this paper is to use multi task le...   \n",
       "\n",
       "                                                   text  \n",
       "0     understanding strong dynamics constitutes a co...  \n",
       "1     cuscuton gravity was originally proposed about...  \n",
       "2     a recent milestone in observational cosmology ...  \n",
       "3     one of the attractive features of the seesaw m...  \n",
       "4     within the usual wisdom it is quite intuitive ...  \n",
       "...                                                 ...  \n",
       "9218  joint extraction of entities and relations is ...  \n",
       "9219  a two sample test detects if two sets of data ...  \n",
       "9220  deformations of special relativity dsr alleged...  \n",
       "9221  autonomous cars are currently primed for mass ...  \n",
       "9222  slot filling models are a useful method for si...  \n",
       "\n",
       "[9223 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"SSN/SSN_Dataset_Short_Clean_2555_NoStop.json\"\n",
    "dataset_path = DATASET+DATA_DIR\n",
    "df = pd.read_json(dataset_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = []\n",
    "\n",
    "# t_max= 2080\n",
    "# a_max=  87\n",
    "# t_min=  1473\n",
    "# a_min=  63\n",
    "# for i, row in df.iterrows():\n",
    "#     t_word = len(row[\"text\"].split())\n",
    "#     a_word = len(row[\"abstract\"].split())\n",
    "#     if ((t_word > t_max or t_word < t_min) or (a_word > a_max or a_word < a_min)):\n",
    "#         rows.append(i)\n",
    "# df.drop(rows, inplace=True)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_json(\"./Dataset/SSN/SSN_Dataset_Short_Clean_2555.json\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # intially, set both the IDs and words to dictionaries with special tokens\n",
    "        self.word2idx = {'<start>': 0, '<end>': 1, '<pad>':2, '<unk>':3}\n",
    "        self.idx2word = {0: '<start>', 1: '<end>', 2: '<pad>', 3: '<unk>'}\n",
    "        self.idx = 4\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # if the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            # this will convert each word to index and index to word as you saw in the tutorials\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # if we try to access a word not in the dictionary, return the id for <unk>\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    ## added function for utility\n",
    "    def get_word(self,index):\n",
    "        # this returns the word when given an index\n",
    "        return self.idx2word[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    \"\"\" \n",
    "    Parses training set token file captions and builds a Vocabulary object and dataframe for \n",
    "    the image and caption data\n",
    "\n",
    "    Returns:\n",
    "        vocab (Vocabulary): Vocabulary object containing all words appearing more than min_frequency\n",
    "    \"\"\"\n",
    "    MIN_FREQUENCY = 0\n",
    "    word_mapping = Counter()\n",
    "\n",
    "    # for index in df.index:\n",
    "    for text in tqdm(data):\n",
    "        for word in text.split():\n",
    "            #print(word)\n",
    "            if word in word_mapping:\n",
    "                word_mapping[word] += 1\n",
    "            else:\n",
    "                word_mapping[word] = 1\n",
    "\n",
    "    # create a vocab instance\n",
    "    vocab = Vocabulary()\n",
    "\n",
    "    # add the words to the vocabulary\n",
    "    for word in word_mapping:\n",
    "        if word_mapping[word] > MIN_FREQUENCY:\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DF to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_txt = df['text']\n",
    "# text_txt.to_csv(\"text_NoStop.txt\", header=False,index=False)\n",
    "# # write to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_model = fasttext.train_unsupervised('text_NoStop.txt', minn=2, epoch=10)\n",
    "# vocab_model.save_model(\"fastText_NoStop.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7835723"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(vocab_model.get_words)\n",
    "# import numpy as np\n",
    "# import gc\n",
    "# import sys\n",
    "\n",
    "# def get_obj_size(obj):\n",
    "#     marked = {id(obj)}\n",
    "#     obj_q = [obj]\n",
    "#     sz = 0\n",
    "\n",
    "#     while obj_q:\n",
    "#         sz += sum(map(sys.getsizeof, obj_q))\n",
    "\n",
    "#         # Lookup all the object referred to by the object in obj_q.\n",
    "#         # See: https://docs.python.org/3.7/library/gc.html#gc.get_referents\n",
    "#         all_refr = ((id(o), o) for o in gc.get_referents(*obj_q))\n",
    "\n",
    "#         # Filter object that are already marked.\n",
    "#         # Using dict notation will prevent repeated objects.\n",
    "#         new_refr = {o_id: o for o_id, o in all_refr if o_id not in marked and not isinstance(o, type)}\n",
    "\n",
    "#         # The new obj_q will be the ones that were not marked,\n",
    "#         # and we will update marked with their ids so we will\n",
    "#         # not traverse them again.\n",
    "#         obj_q = new_refr.values()\n",
    "#         marked.update(new_refr.keys())\n",
    "\n",
    "#     return sz\n",
    "# x = np.random.rand(1024).astype(np.float64)\n",
    "# y = np.random.rand(1024).astype(np.float64)\n",
    "# a = {'x': x, 'y': y}\n",
    "# get_obj_size(vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384a723d9b624cb09fc5d08ba85106e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abstract_vocab = build_vocab(df[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b267c2dbbbb64c36a0817984aa3bbc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_vocab = build_vocab(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(vocab, text):\n",
    "    word_ids = [vocab(\"<start>\")]\n",
    "    for word in text.split():\n",
    "        word_ids.append(vocab(word))\n",
    "    word_ids.append(vocab(\"<end>\"))\n",
    "#     while len(word_ids) < max_len:\n",
    "#             word_ids.append(vocab(\"<pad>\"))\n",
    "    return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACT_MAX = 150\n",
    "TEXT_MAX = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSNDataset(Dataset):\n",
    "    def __init__(self, df, a_vocab, t_vocab):\n",
    "\n",
    "        self.df = df\n",
    "        self.a_vocab = a_vocab\n",
    "        self.t_vocab = t_vocab\n",
    "        self.abstract_max_len = ABSTRACT_MAX\n",
    "        self.text_max_len = TEXT_MAX\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return word_id\n",
    "        abstract = self.df.iloc[index][\"abstract\"]\n",
    "        text = self.df.iloc[index][\"text\"]\n",
    "        \n",
    "        a_word_ids = tokenise(self.a_vocab, abstract)\n",
    "        t_word_ids = tokenise(self.t_vocab, text)\n",
    "\n",
    "        a_length = len(a_word_ids)\n",
    "        t_length = len(t_word_ids)\n",
    "    \n",
    "        return a_word_ids, t_word_ids# torch.tensor(t_length)# torch.tensor(t_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_list(lis):\n",
    "    list_len = [len(i) for i in lis]\n",
    "    return (max(list_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_collate_fn(data):\n",
    "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    Args:\n",
    "        data: list of tuple of 2 word ids\n",
    "        - abstract id\n",
    "        - text id\n",
    "    Returns:\n",
    "        abstract list ids\n",
    "        text list ids\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    abstracts, texts = zip(*data)\n",
    "    abstract_list = []\n",
    "    text_list = []\n",
    "    for i in range(len(texts)):\n",
    "        while len(texts[i]) < find_max_list(texts):\n",
    "            texts[i].append(text_vocab(\"<pad>\"))\n",
    "        text_list.append(torch.tensor(texts[i]))\n",
    "        \n",
    "    for i in range(len(abstracts)):\n",
    "     \n",
    "        while len(abstracts[i]) < ABSTRACT_MAX:\n",
    "            abstracts[i].append(abstract_vocab(\"<pad>\"))\n",
    "        abstract_list.append(torch.tensor(abstracts[i]))\n",
    "\n",
    "\n",
    "#     abstracts = torch.tensor(abstracts)\n",
    "#     texts = torch.tensor(texts)\n",
    "#     abstracts = [ torch.Tensor(abstract).to(device) for abstract in abstracts ]\n",
    "# if batch size is 1 then use [0]\n",
    "    abstracts = torch.stack(tuple(abstract_list), 0)\n",
    "    #abstracts = abstracts.unsqueeze(0)\n",
    "#     abstracts = torch.nn.utils.rnn.pad_sequence(abstracts)\n",
    "    texts = torch.stack(tuple(text_list), 0)\n",
    "    #texts = texts.unsqueeze(0)\n",
    "\n",
    "    return abstracts, texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, random_state=seed, train_size = 0.7)\n",
    "train_data, valid_data = train_test_split(train_data, random_state=seed, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SSNDataset(train_data, abstract_vocab, text_vocab)\n",
    "valid_set = SSNDataset(valid_data, abstract_vocab, text_vocab)\n",
    "test_set = SSNDataset(test_data, abstract_vocab, text_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=2, shuffle=True, collate_fn=text_collate_fn) # num_worker can't be 2+ as the time it \n",
    "                                                                 # takes to build iter is much longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(valid_set, batch_size=2, shuffle=True, collate_fn=text_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667dacafe30249068efae4685e26c3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_set = SSNDataset(df, abstract_vocab, text_vocab)\n",
    "all_loader = DataLoader(all_set, batch_size=1, shuffle=True, collate_fn=text_collate_fn)\n",
    "abstracts = []\n",
    "texts = []\n",
    "for i, (abstract, text) in tqdm(enumerate(all_loader)):\n",
    "    a_size = list(abstract.size())[1]\n",
    "\n",
    "    t_size = list(text.size())[1]\n",
    "    abstracts.append(a_size)\n",
    "    texts.append(t_size)\n",
    "numbers = list(range(len(all_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length for abstract:  260\n",
      "max length for text:  5733\n",
      "min length for abstract:  3\n",
      "min length for text:  14\n",
      "average length for abstract:  86\n",
      "average length for text:  2030\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(\"max length for abstract: \",max(abstracts))\n",
    "print(\"max length for text: \",max(texts))\n",
    "print(\"min length for abstract: \",min(abstracts))\n",
    "print(\"min length for text: \",min(texts))\n",
    "print(\"average length for abstract: \",math.floor(mean(abstracts)))\n",
    "print(\"average length for text: \",math.floor(mean(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile:  2080.0\n",
      "50 percentile:  87.0\n",
      "0.08 percentile:  1473.0\n",
      "0.08 percentile:  63.0\n"
     ]
    }
   ],
   "source": [
    "print(\"50 percentile: \", np.percentile(texts, 55))\n",
    "print(\"50 percentile: \", np.percentile(abstracts, 55))\n",
    "print(\"0.08 percentile: \", np.percentile(texts, 25))\n",
    "print(\"0.08 percentile: \", np.percentile(abstracts, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_perc = []\n",
    "t_perc = []\n",
    "for i in range(100):\n",
    "    i +=1\n",
    "    a_perc.append(np.percentile(abstracts, i))\n",
    "    t_perc.append(np.percentile(texts,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAS4UlEQVR4nO3df6zddX3H8eebcodX2SyMQtpLWZmpVdBIlxvixrKgLCuiWdFEVxINf5DgHxhxMZ2t/qFb0khSxS3ZNKviZFNBohUaY6yIGKOZYPkR+VE6O0HobUfr9E4iN1jKe3+c78Uvt+fce+49v+75nOcjubnnfM73nPv+0PZ1P7y/n+85kZlIkspyyqALkCR1n+EuSQUy3CWpQIa7JBXIcJekAp066AIAzjrrrFy3bt2gy5CkoXLffff9IjNXNXtsWYT7unXr2Ldv36DLkKShEhE/b/WYbRlJKpDhLkkFMtwlqUCGuyQVyHCXpAIti90ykjRqbn9gip17D3B4eoY1K8fZumkDV26c6NrrG+6S1Ge3PzDF9t0PMXP8BABT0zNs3/0QQNcC3raMJPXZzr0HXgz2WTPHT7Bz74Gu/QzDXZL67PD0zKLGl8Jwl6Q+W7NyfFHjS2G4S1Kfbd20gfGxFS8ZGx9bwdZNG7r2MzyhKkl9NnvS1N0yklSYKzdOdDXM57ItI0kFWjDcI2JtRNwdEfsj4pGIuL4a/1hETEXEg9XXFbXnbI+IgxFxICI29XICkqSTtdOWeR74YGbeHxG/D9wXEXdWj30qMz9RPzgiLgC2ABcCa4DvRMSrM/OlmzolST2z4Mo9M49k5v3V7WeA/cB8jaLNwK2Z+VxmPg4cBC7uRrGSpPYsquceEeuAjcA91dD7IuInEfH5iDijGpsAnqo97RBNfhlExLURsS8i9h07dmzxlUuSWmo73CPidOBrwAcy89fAZ4BXARcBR4BPzh7a5Ol50kDmrsyczMzJVauafgSgJGmJ2gr3iBijEexfyszdAJn5dGaeyMwXgM/yu9bLIWBt7ennAoe7V7IkaSHt7JYJ4CZgf2beWBtfXTvs7cDD1e09wJaIOC0izgfWA/d2r2RJ0kLa2S1zCfAe4KGIeLAa+zBwVURcRKPl8gTwXoDMfCQibgMepbHT5jp3ykhSfy0Y7pn5A5r30b85z3N2ADs6qEuS1AGvUJWkAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUoAXDPSLWRsTdEbE/Ih6JiOur8TMj4s6I+Gn1/Yzac7ZHxMGIOBARm3o5AUnSydpZuT8PfDAzXwu8EbguIi4AtgF3ZeZ64K7qPtVjW4ALgcuBT0fEil4UL0lqbsFwz8wjmXl/dfsZYD8wAWwGbq4Ouxm4srq9Gbg1M5/LzMeBg8DFXa5bkjSPRfXcI2IdsBG4BzgnM49A4xcAcHZ12ATwVO1ph6qxua91bUTsi4h9x44dW0LpkqRW2g73iDgd+Brwgcz89XyHNhnLkwYyd2XmZGZOrlq1qt0yJEltaCvcI2KMRrB/KTN3V8NPR8Tq6vHVwNFq/BCwtvb0c4HD3SlXktSOdnbLBHATsD8zb6w9tAe4urp9NXBHbXxLRJwWEecD64F7u1eyJGkhp7ZxzCXAe4CHIuLBauzDwA3AbRFxDfAk8E6AzHwkIm4DHqWx0+a6zDzR7cIlSa0tGO6Z+QOa99EBLmvxnB3Ajg7qkiR1wCtUJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVqJ3PUJUkdcHtD0yxc+8BDk/PsGblOFs3beDKjRM9+VmGuyT1we0PTLF990PMHD8BwNT0DNt3PwTQk4C3LSNJfbBz74EXg33WzPET7Nx7oCc/z3CXpD44PD2zqPFOGe6S1AdrVo4varxThrsk9cHWTRsYH1vxkrHxsRVs3bShJz/PE6qS1AezJ03dLSNJhbly40TPwnwu2zKSVCBX7pLUQ/28cKnOcJekHun3hUt1tmUkqUf6feFSneEuST3S7wuX6gx3SeqRfl+4VGe4S1KP9PvCpTpPqEpSl9V3yLxyfIyXjZ3C9LPH3S0jScNq7g6Z6ZnjjI+t4FN/c1HfLmCCNtoyEfH5iDgaEQ/Xxj4WEVMR8WD1dUXtse0RcTAiDkTEpl4VLknL0SB3yNS103P/AnB5k/FPZeZF1dc3ASLiAmALcGH1nE9HxIomz5WkIg1yh0zdguGemd8Hftnm620Gbs3M5zLzceAgcHEH9UnSUBnkDpm6TnbLvC8iflK1bc6oxiaAp2rHHKrGThIR10bEvojYd+zYsQ7KkKTBu/2BKS654btMTc8Qcx7r1w6ZuqWG+2eAVwEXAUeAT1bjc+cEkM1eIDN3ZeZkZk6uWrVqiWVI0uDNnkSdqlovye/CcGLlOB9/x+v7ejIVlrhbJjOfnr0dEZ8FvlHdPQSsrR16LnB4ydVJ0hBodhI1aQT7D7e9eSA1LWnlHhGra3ffDszupNkDbImI0yLifGA9cG9nJUrS8lRvxTTT75OodQuu3CPiFuBS4KyIOAR8FLg0Ii6i8cvpCeC9AJn5SETcBjwKPA9cl5knmrysJA21ufvZm+n3SdS6BcM9M69qMnzTPMfvAHZ0UpQkLXfNWjF1gziJWucVqpK0BPO1XCb6+DYDrRjuktSm+nvGnBLBiTx5M+AgT6LWGe6S1Ia5PfZmwT7oVkyd4S5J85hdrbfaEbMighcy+/qOj+0w3CWphXZ2xLyQyeM3vLWPVbXHD+uQpBYW2hEDg93uOB9X7pI0x0KtmFnLqcc+l+EuSTXttGJgeWx3nI/hLkksbrU+iDcCWyzDXdLIqgd60OItbGuW+2q9znCXNJLmtl/aCfblcHFSuwx3SSOl3fZL3XI+cdqK4S5pZLR7srRumFoxdYa7pOItdbU+DCdOWzHcJRVtMav12ZOqw7parzPcJRVpsav1EgK9znCXVIzFbm2E4W+/tGK4SyrCYrc2Qnmr9TrDXdJQG8WTpe0w3CUNnaW0X2aVvFqvM9wlDYVWgd5usI/Car3OcJe07C2lnw5lbW1cLMNd0rJR/wDqNSvHedNrVnH3Y8cW1U+fNYqBXme4SxqoVu2WqekZvvijJxf9eqPWfmnFcJc0MEttt8w1yu2XVgx3SX23lO2Lcxno8zPcJfVFJ9sX5zLQF2a4S+qZTrcvzmU/vX2Gu6Su6jTQ6+2W2d0ys7tnXK23z3CX1LFurdBtt3SP4S5pSbrZcrHd0n2Gu6S2dTPQ3e3SW4a7pHkZ6MPJcJd0EgN9+C0Y7hHxeeBtwNHMfF01dibwFWAd8ATwrsz8VfXYduAa4ATw/szc25PKJXWVgV6WdlbuXwD+Gfj32tg24K7MvCEitlX3PxQRFwBbgAuBNcB3IuLVmbnwJ9NK6ov6m3O9cnyMCPjVs8cN9MIsGO6Z+f2IWDdneDNwaXX7ZuB7wIeq8Vsz8zng8Yg4CFwM/GeX6pW0SHPD/De/fZ7jJxrxPT1z/MXjDPSyLLXnfk5mHgHIzCMRcXY1PgH8qHbcoWpMUh+1arHUw3ypDPTh0O0TqtFkrOmCICKuBa4FOO+887pchjR6un2pf52BPnyWGu5PR8TqatW+GjhajR8C1taOOxc43OwFMnMXsAtgcnKyG3//pJFjoKuVpYb7HuBq4Ibq+x218S9HxI00TqiuB+7ttEhJv2Ogqx3tbIW8hcbJ07Mi4hDwURqhfltEXAM8CbwTIDMfiYjbgEeB54Hr3Ckjda6bgT52SnD6y05l+tnjL+6WmX72uG/MVZh2dstc1eKhy1ocvwPY0UlR0ihptjWxHrxuU9RSeIWqNADt7GZxm6I6YbhLfdLLXvksA12zDHephwx0DYrhLnWZga7lwHCXlmC+k6D1y/sNdA2K4S61abEnQRdj9vVWNtkt4zZFLYXhLs3Rq3dNnMuVuHrJcNfIaifEO92OOJeBrn4x3DVS2mmtdPuNjgx0DYLhruL1Y/dKXf3yfnvlGhTDXcXoV698VrOToIa5lgvDXUOt320WWywaFoa7hoKrcmlxDHctG+1eGNTNVbkhrlIZ7hqoXl4Y1IqtFY0Cw1191+/dK2Cga/QY7uqLfgW6bRapwXBXV/X7xCe4KpeaMdzVsX5sR/RzP6XFMdzVtn6sym2rSN1huOsk/X5DLdsqUvcZ7gK80lMqjeE+ouauznv16UF1BrrUP4b7CGlndd4N9s2lwTPcC+R2REmGeyH61TN3VS4NB8N9yPT78z0NcWk4Ge5DoN8XCRni0vAz3JepfrwXi31yqVyG+zJioEvqFsN9wHoV6PbMpdFmuA9ArwPdVbkkw71PuhnorsolLcRw76L6NsU1K8d502tWcfdjx7oa6K7KJbXDcO9QqxX51PQMX/zRky8eZ6BL6qeOwj0ingCeAU4Az2fmZEScCXwFWAc8AbwrM3/VWZnLSy93tRjokrqhGyv3N2XmL2r3twF3ZeYNEbGtuv+hLvycgTLQJQ2TXrRlNgOXVrdvBr7HkIa7gS5pWHUa7gl8OyIS+NfM3AWck5lHADLzSESc3eyJEXEtcC3Aeeed12EZ3WOgSypBp+F+SWYergL8zoh4rN0nVr8IdgFMTk726vMh2tKLbYoTtd0ys7tnDHRJ/dJRuGfm4er70Yj4OnAx8HRErK5W7auBo12os+t6FegGuKTlYMnhHhGvAE7JzGeq238F/AOwB7gauKH6fkc3Cu2m2x+YYvvuh5g5fgIw0CWVp5OV+znA1yNi9nW+nJnfiogfA7dFxDXAk8A7Oy+zO+qr9aUw0CUNiyWHe2b+DHhDk/H/BS7rpKhuatV+aZeBLmkYFX2F6lLbLwa6pGFXZLgvpf1ioEsqSXHhPne13g4DXVJpign3pazWx8dW8PF3vN5Ql1ScIsJ9Mat12y+SRkER4b5z74G2gt1AlzQqhjrc223F2H6RNGqGNtzbbcW4Wpc0ioY23BdqxbhalzTKhjbcD8/TinG1LmnUDW24r1k53rTXPrFynB9ue/MAKpKk5eOUQRewVFs3bWB8bMVLxsbHVrB104YBVSRJy8fQrtxnWy479x7wwzAkaY6hDXdoBLxhLkknG9q2jCSpNcNdkgpkuEtSgQx3SSqQ4S5JBYrMxX6qaA+KiDgG/LyDlzgL+EWXyhkWozhnGM15O+fRsdh5/1Fmrmr2wLII905FxL7MnBx0Hf00inOG0Zy3cx4d3Zy3bRlJKpDhLkkFKiXcdw26gAEYxTnDaM7bOY+Ors27iJ67JOmlSlm5S5JqDHdJKtBQh3tEXB4RByLiYERsG3Q9vRARayPi7ojYHxGPRMT11fiZEXFnRPy0+n7GoGvthYhYEREPRMQ3qvtFzzsiVkbEVyPiserP/E9LnzNARPxt9ff74Yi4JSJeVuK8I+LzEXE0Ih6ujbWcZ0Rsr/LtQERsWszPGtpwj4gVwL8AbwEuAK6KiAsGW1VPPA98MDNfC7wRuK6a5zbgrsxcD9xV3S/R9cD+2v3S5/1PwLcy8zXAG2jMveg5R8QE8H5gMjNfB6wAtlDmvL8AXD5nrOk8q3/nW4ALq+d8usq9tgxtuAMXAwcz82eZ+VvgVmDzgGvqusw8kpn3V7efofGPfYLGXG+uDrsZuHIgBfZQRJwLvBX4XG242HlHxB8AfwHcBJCZv83MaQqec82pwHhEnAq8HDhMgfPOzO8Dv5wz3Gqem4FbM/O5zHwcOEgj99oyzOE+ATxVu3+oGitWRKwDNgL3AOdk5hFo/AIAzh5gab3yj8DfAS/Uxkqe9x8Dx4B/q1pRn4uIV1D2nMnMKeATwJPAEeD/MvPbFD7vmlbz7Cjjhjnco8lYsfs6I+J04GvABzLz14Oup9ci4m3A0cy8b9C19NGpwJ8An8nMjcBvKKMVMa+qx7wZOB9YA7wiIt492KqWhY4ybpjD/RCwtnb/XBr/K1eciBijEexfyszd1fDTEbG6enw1cHRQ9fXIJcBfR8QTNFpub46IL1L2vA8BhzLznur+V2mEfclzBvhL4PHMPJaZx4HdwJ9R/rxntZpnRxk3zOH+Y2B9RJwfEb9H48TDngHX1HURETR6sPsz88baQ3uAq6vbVwN39Lu2XsrM7Zl5bmauo/Fn+93MfDcFzzsz/wd4KiI2VEOXAY9S8JwrTwJvjIiXV3/fL6Nxbqn0ec9qNc89wJaIOC0izgfWA/e2/aqZObRfwBXAfwH/DXxk0PX0aI5/TuN/xX4CPFh9XQH8IY0z6z+tvp856Fp7+N/gUuAb1e2i5w1cBOyr/rxvB84ofc7VvP8eeAx4GPgP4LQS5w3cQuO8wnEaK/Nr5psn8JEq3w4Ab1nMz/LtBySpQMPclpEktWC4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAL9PxWRLUR2nteWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(a_perc)),a_perc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(t_perc)), t_perc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, num_layer, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=voc_size,\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layer,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "#     def init_hidden(self, batch_size):\n",
    "#         return (torch.zeros(self.num_layer, batch_size, self.hidden_dim, device=device),\n",
    "#                 torch.zeros(self.num_layer, batch_size, self.hidden_dim, device=device))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #start = time.time()\n",
    "\n",
    "        #print(input.shape)\n",
    "        embedded = self.embeddings(input)\n",
    "        lengths = (input != 2).sum(1) # pad index = 2\n",
    "        #print(lengths)\n",
    "        batch_size = input.shape[0]\n",
    "        embedded = self.embeddings(input)\n",
    "        #print(\"lengths: \",lengths.shape)\n",
    "#         print(lengths)\n",
    "#         print(\"embedded: \",embedded.shape)\n",
    "#         print(\"embedded: \",embedded.view(1, 1, -1).shape)\n",
    "    \n",
    "#         print(\"initial_hidden\", initial_hidden.shape)\n",
    "        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), \n",
    "                                                           batch_first=True,enforce_sorted=False)\n",
    "#         print(\"embedded: \",embedded[0].shape)\n",
    "#         print(\"embedded: \",embedded[1].shape)\n",
    "        output, hidden = self.lstm(embedded)\n",
    "        #print(\"output: \",output.shape)\n",
    "        output, lengths = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True,padding_value=2)\n",
    "        lengths = lengths.to(device)\n",
    "        return output, hidden, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, voc_size, num_layer,hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=voc_size,\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "        self.attention = Attention(hidden_dim,hidden_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim*2,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layer,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hidden_dim * 2,\n",
    "            out_features=voc_size\n",
    "        )\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "\n",
    "    def forward(self, input, hidden, context, encoder_outputs, lengths): # also add in encoder outputs \n",
    "                                                                # to create context\n",
    "        #start = time.time()\n",
    "        embedded = self.embeddings(input) # [batch, 256]\n",
    "        # hidden = [bathc, seq_len, hidden_dim]\n",
    "        # print(hidden[0])\n",
    "        #hidden_permute = hidden.permute(0,1) # [1, 512]\n",
    "        decoder_input = torch.cat((embedded, context), -1).unsqueeze(1) # [batch, 1, 512]\n",
    "        decoder_output, hidden = self.lstm(decoder_input, hidden)\n",
    "        decoder_output = decoder_output.squeeze(1)\n",
    "        \n",
    "        # mask to block decoder from knowing future sequences/block padding\n",
    "        max_len = lengths.max().item()\n",
    "        #print(\"max len: \", max_len)\n",
    "        \n",
    "        \n",
    "        mask = torch.arange(max_len).expand(len(lengths), max_len) < lengths.unsqueeze(1).cpu()\n",
    "        mask = mask.to(device)\n",
    "#         print(\"size: \", len(mask))\n",
    "#         print(\"mask 0\",mask[0])\n",
    "#         print(\"mask0 len\",len(mask[0]))\n",
    "#         print(\"mask0 = true\",torch.sum(mask[0] == True))\n",
    "#         print(\"mask1\",mask[1])\n",
    "#         print(\"mask1 len\",len(mask[1]))\n",
    "#         print(\"mask1 = true\",torch.sum(mask[1] == True))\n",
    "        attention = self.attention(decoder_output, encoder_outputs, mask).to(device)\n",
    "\n",
    "        context = attention.unsqueeze(1).bmm(encoder_outputs).squeeze(1)\n",
    "        #print(\"context shape: \", context.shape)\n",
    "        output_context = torch.cat((decoder_output, context), dim=1) # []\n",
    "        #output_context = torch.nn.functional.relu(output_context)\n",
    "        \n",
    "        output_context = self.fc(output_context)\n",
    "        #print(output[0])\n",
    "        output = self.logsoftmax(output_context)\n",
    "        #end = time.time()\n",
    "        #print(\"Decoder Time: \",end - start)\n",
    "#         print(\"decode output: \",output)\n",
    "#         print(\"output: \", output.shape)\n",
    "        # output is [batch, seq_len, hidden_dim]\n",
    "        # hidden = 2,2,256 [2, batch, hidden_dim]\n",
    "        return output, hidden, context, attention\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.attn_hidden_vector = nn.Linear(encoder_hidden_dim + decoder_hidden_dim, decoder_hidden_dim)\n",
    "        \n",
    "#         self.attn_scoring_fn = nn.Linear(decoder_hidden_dim, decoder_hidden_dim, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden = [num_layer, batch size, decoder hidden dim]\n",
    "        # encoder_outputs [batch, seq_len, hidden_dim]\n",
    "        seq_len = encoder_outputs.shape[1] # 10000\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        \n",
    "        #attention_scores = torch.zeros((batch_size, seq_len), device=device)\n",
    "\n",
    "        hidden = hidden.repeat(seq_len,1,1)\n",
    "        #print(hidden.shape)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        #print(encoder_outputs.shape)\n",
    "\n",
    "        #print(\"attention score: \", attention_scores.shape)\n",
    "#         print(\"hidden atten: \",hidden.shape)\n",
    "#         print(\"encoder output: \", encoder_outputs.shape)\n",
    "\n",
    "\n",
    "        attention_scores = torch.einsum('sbd,sbd->bs',hidden, encoder_outputs) \n",
    "        # need to check if can use just lbd, sbd ->bs because hidden repeat don't work if more than 1 layer\n",
    "        #print(\"attention_scores: \",attention_scores.shape)\n",
    "\n",
    "            \n",
    "        attention_scores[~mask] = -float('inf')\n",
    "        #print(attention_scores)\n",
    "        #attention_scores = nn.functional.relu(attention_scores)\n",
    "        #end = time.time()\n",
    "        #print(\"Attention Time: \",end - start)\n",
    "        attention_scores = nn.functional.softmax(attention_scores, dim=1)\n",
    "        return attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, text_size, abstract_size, num_layer, hidden_dim, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(text_size, num_layer=num_layer, hidden_dim=hidden_dim).to(device)\n",
    "        self.decoder = Decoder(abstract_size, num_layer=num_layer, hidden_dim=hidden_dim).to(device)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, text, max_len=ABSTRACT_MAX,abstract=None, temperature=0.5):\n",
    "        #decoder = Decoder(len(abstract_vocab), 512).to(device)\n",
    "        output, hidden, lengths = self.encoder(text) # [10000, 512] [seq_len, hidden_dim]\n",
    "        batch_size = output.shape[0]\n",
    "#         print(text)\n",
    "#         print(max_len)\n",
    "#         print(abstract)\n",
    "#         print(temperature)\n",
    "        decoder_hidden = hidden\n",
    "        context = torch.zeros(batch_size, self.hidden_dim).to(device) # batch, hidden_dim\n",
    "\n",
    "        decoder_input = text[:,:1].squeeze(1).long().to(device) # select first elements\n",
    "#         out1 = output[0] # varied\n",
    "#         out2 = output[1]\n",
    "#         out3 = output[2]\n",
    "        done = torch.BoolTensor(batch_size).fill_(False).to(device)\n",
    "        outputs = []\n",
    "        words = []\n",
    "        for t in range(max_len):\n",
    "            \n",
    "            decoder_output, decoder_hidden, context, attention = self.decoder(decoder_input, \n",
    "                                                            decoder_hidden, context, output, lengths)\n",
    "            \n",
    "            outputs.append(decoder_output)\n",
    "            \n",
    "            decoder_input = decoder_output.argmax(1)\n",
    "            \n",
    "            words.append(decoder_input)\n",
    "\n",
    "        sentence = \"\"\n",
    "        \n",
    "#         for i in words:\n",
    "#             sentence += abstract_vocab.get_word(i[0].item()) + \" \"\n",
    "#         print(\"predict:\", sentence)\n",
    "#         sentence = \"\"\n",
    "#         for i in abstract[0]:\n",
    "#             sentence += abstract_vocab.get_word(i.item()) + \" \"\n",
    "#         print(\"actual:\", sentence)\n",
    "\n",
    "        outputs = torch.stack(outputs, 1) # B x T x D\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(len(text_vocab),len(abstract_vocab),2,512, device)\n",
    "model = model.to(device)\n",
    "#out1 = model(abstract, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "~~1 - Complete model definition~~ <br>\n",
    "~~2 - fix word vocab~~ <br>\n",
    "~~3 - Define Attention model and combine with Decoder~~ <br>\n",
    "    ~~1 - Adjust to allow for batch ~~<br>\n",
    "4 - trace through program to see where it starts to have the same tensor <br>\n",
    "~~5 - use mask to hide paddings~~ switch on off to see difference<br>\n",
    "~~6 - Switch from GRU to LSTM~~<br>\n",
    "~~7 - Add training script to train ~~<br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model):\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # learning rate also affects overfitting\n",
    "    epoch_size = 5\n",
    "    loss_epoch = []\n",
    "    loss_accuracy = []\n",
    "    eval_loss_epoch = []\n",
    "    eval_accuracy = []\n",
    "    best_loss = 10000\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoch_size):\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        loss_batch = []\n",
    "        batch_acc = []\n",
    "        for i, (abstract, text) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            # send batch data to device\n",
    "            abstract = abstract.to(device)\n",
    "            text = text.to(device)\n",
    "            #print(text)\n",
    "            outputs = model(text, abstract=abstract).to(device).permute(0,2,1)\n",
    "            #print(\"output\",outputs)\n",
    "#             print(outputs.dtype)\n",
    "#             print(\"abstract\", abstract[:,0])\n",
    "#             print(abstract.dtype)\n",
    "            \n",
    "            loss = criterion(outputs, abstract)\n",
    "            batch_acc.append(get_accuracy(outputs, abstract))\n",
    "            #print(loss.item())\n",
    "\n",
    "            # Optimise\n",
    "            del abstract\n",
    "            del text\n",
    "            del outputs\n",
    "\n",
    "            loss_batch.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "           # plot_grad_flow(model.named_parameters())\n",
    "\n",
    "            del loss\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            #print(loss.item())\n",
    "        current_loss = np.average(loss_batch)\n",
    "        current_acc = np.average(batch_acc)\n",
    "        loss_epoch.append(current_loss)\n",
    "        loss_accuracy.append(current_acc)\n",
    "\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}, Acc: {:.2f}'\n",
    "            .format(epoch+1, epoch_size, current_loss, current_acc*100))\n",
    "\n",
    "        eval_loss_batch = []\n",
    "        eval_batch_acc = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, (abstract, text) in enumerate(valid_loader):\n",
    "                abstract = abstract.to(device)\n",
    "                text = text.to(device)\n",
    "\n",
    "                scores = model(text, abstract=abstract).permute(0,2,1)\n",
    "\n",
    "                eval_loss = criterion(scores, abstract)\n",
    "                #print(eval_loss.item())\n",
    "                eval_loss_batch.append(eval_loss.item())\n",
    "                eval_batch_acc.append(get_accuracy(scores, abstract))\n",
    "                del abstract\n",
    "                del text\n",
    "                del scores\n",
    "                del eval_loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        current_eval_loss = np.average(eval_loss_batch)\n",
    "        current_eval_acc = np.average(eval_batch_acc)\n",
    "        \n",
    "        eval_loss_epoch.append(current_eval_loss)\n",
    "        eval_accuracy.append(current_eval_acc)\n",
    "        \n",
    "        if (current_eval_loss < best_loss):\n",
    "            best_loss = current_eval_loss\n",
    "            torch.save(model.state_dict(), 'best_decoder'+str(epoch+1)+'.pth')\n",
    "            print(\"Best eval loss updated!\")\n",
    "\n",
    "        print('Valid Epoch [{}/{}], Loss: {:.4f}, Acc: {:.2f}'\n",
    "            .format(epoch+1, epoch_size, current_eval_loss, current_eval_acc*100))\n",
    "    return loss_epoch, eval_loss_epoch, loss_accuracy, eval_accuracy\n",
    "def get_accuracy(preds, abstracts):\n",
    "#     print(\"preds: \", preds.shape)\n",
    "#     print(\"abstracts:\", abstracts.shape)\n",
    "    pred_ids = preds.argmax(1)\n",
    "    \n",
    "    correct_preds = (pred_ids == abstracts).sum().item()\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_samples = abstracts.shape[0] * abstracts.shape[1]\n",
    "\n",
    "    # Calculate accuracy as the ratio of correct predictions to total samples\n",
    "    accuracy = correct_preds / total_samples\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8061d94f03524c6cbd6bb4f3a698a55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2582 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 8.00 GiB total capacity; 5.85 GiB already allocated; 0 bytes free; 7.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [228]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss, eval_loss, train_acc, eval_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [227]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, valid_loader, model)\u001b[0m\n\u001b[0;32m     21\u001b[0m             text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;66;03m#print(text)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabstract\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabstract\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m             \u001b[38;5;66;03m#print(\"output\",outputs)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#             print(outputs.dtype)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#             print(\"abstract\", abstract[:,0])\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#             print(abstract.dtype)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m             loss \u001b[38;5;241m=\u001b[39m criterion(outputs, abstract)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [216]\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, text, max_len, abstract, temperature)\u001b[0m\n\u001b[0;32m     25\u001b[0m words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[1;32m---> 28\u001b[0m     decoder_output, decoder_hidden, context, attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(decoder_output)\n\u001b[0;32m     33\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m decoder_output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [214]\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, input, hidden, context, encoder_outputs, lengths)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# hidden = [bathc, seq_len, hidden_dim]\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# print(hidden[0])\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#hidden_permute = hidden.permute(0,1) # [1, 512]\u001b[39;00m\n\u001b[0;32m     30\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded, context), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [batch, 1, 512]\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m decoder_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m decoder_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# mask to block decoder from knowing future sequences/block padding\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 8.00 GiB total capacity; 5.85 GiB already allocated; 0 bytes free; 7.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_loss, eval_loss, train_acc, eval_acc = train(train_loader, valid_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def writeLoss(filename, loss):\n",
    "    # filename in format losstype+layer+dim, validL2D256\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = filename + \"_\" + timestr + \".txt\"\n",
    "    with open(filename, \"a\") as myfile:\n",
    "        for item in items:\n",
    "            myfile.write(item+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeLoss(\"Train\", train_loss)\n",
    "writeLoss(\"Valid\", eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_decoder20230719.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        print(n)\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            #print(type(p.grad))\n",
    "            ave_grads.append(torch.mean(torch.abs(p.grad)).cpu())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f8bb119ac0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA35klEQVR4nO3dd3hUVfrA8e+b3ntCCYQEUHoPSAfFXhCssGsvrG7XtW5z3V33x+66u+rqiqjYBbsiIqJIU0DpPYBAgEAgIb23Ob8/zgChBAIkuZPM+3mePJm59869bwJ558w5575HjDEopZTyHj5OB6CUUqppaeJXSikvo4lfKaW8jCZ+pZTyMpr4lVLKy/g5HUB9xMXFmeTkZKfDUEqpZmXlypUHjTHxx25vFok/OTmZFStWOB2GUko1KyKy60TbtatHKaW8jCZ+pZTyMpr4lVLKyzSLPv4TqaqqIiMjg/LycqdDadaCgoJo164d/v7+ToeilGoizTbxZ2RkEB4eTnJyMiLidDjNkjGGnJwcMjIySElJcTocpVQTabZdPeXl5cTGxmrSPwsiQmxsrH5qUsrLNNvED2jSbwD6O1TK+zTrxK+UUs1O0X5Y/jJUlpz62EYqm6+J/wzl5+fzv//974xee/nll5Ofn1/v4//0pz/x5JNPntG1lFIOW/8+fHAXVJbaRP7h3fDZ/fDcYNj8KbhcJ37dnuXw0oWQl97gITXbwV2nHUr8P/3pT4/bV1NTg6+vb52vnT17dmOGppTyFFs+t4neuGzS73QB7FwE590DOxbCOzdBZBL0uRHaD4bYTpC/C9I+g+9fhIi2UHQAopMbNCxt8Z+hRx55hO3bt9O3b18efPBBFixYwPnnn8+PfvQjevXqBcC4ceMYMGAAPXr0YOrUqYdfm5yczMGDB0lPT6dbt27cfffd9OjRg4svvpiysrKTXnfNmjUMHjyY3r17M378ePLy8gB45pln6N69O71792bChAkALFy4kL59+9K3b1/69etHUVFRI/02lGqmqsrhvdttMj5dOxdD+jd179+zHN67Ddr0gZEPwob34dNfQfvz4JL/g3sWw7Uv22S/6El461p4pi+8frVN+oPuhp99B0nnnelPV6cW0eJ//NONbNpX2KDn7N42gseu6lHn/smTJ7NhwwbWrFkDwIIFC/j+++/ZsGHD4amR06ZNIyYmhrKyMgYOHMi1115LbGzsUefZtm0b06dP58UXX+SGG27ggw8+4Kabbqrzurfccgv//e9/GTVqFH/84x95/PHHeeqpp5g8eTI7d+4kMDDwcDfSk08+yXPPPcewYcMoLi4mKCjo7H4pSrU0S5+FjR9CwR6466v6v27tO/DxPRAYDr/eAEERR+8vL4T374CwVvCj9yA0DnK225b8VU+Djw/gA72us1/lBZC5DnJ32NZ9QncIO662WoPRFn8DGjRo0FHz4Z955hn69OnD4MGD2bNnD9u2bTvuNSkpKfTt2xeAAQMGkJ6eXuf5CwoKyM/PZ9SoUQDceuutLFpkWyq9e/fmxz/+MW+++SZ+fvb9fNiwYdx///0888wz5OfnH96uVLNXXWkTZe3Bz7r6yutSkAGL/wXBMZCxHDJW1n1saS6sew8W/hM++w189BNo3csm7O/dn+bz90D6tzaOOY9AYYZt0YfFg4h9fN9GSOh2/PmDIiFlBAy4FTqOatSkDy2kxX+ylnlTCg0NPfx4wYIFfPXVVyxdupSQkBBGjx59wvnygYGBhx/7+vqesqunLp999hmLFi1i5syZ/OUvf2Hjxo088sgjXHHFFcyePZvBgwfz1Vdf0bVr1zM6v1Iew1Vju1C2fAbnXAwD74KVr8KOBfDj9yF5WP3OM/cPtu/9tlnw8iXw3fMQ/Kjtd4/tZLtjSnPg67/A9q/tsQA+/tDtSrjmRXj3Flj6HHQcDdMnQEk2hLWG4v0w4gFoP/DI9Xx8Gj2h11eLSPxOCA8PP2mfeUFBAdHR0YSEhJCWlsayZcvO+pqRkZFER0ezePFiRowYwRtvvMGoUaNwuVzs2bOH888/n+HDh/P2229TXFxMTk4OvXr1olevXixdupS0tDRN/Kp5MwY+f9gm/Z7XwdY5sG0uBEVBcDS8fzv8ZBGEtz75Oeb/zXbxjH4UWvWAfjfB8hftgGtNpZ1Jsy0VqsshJBZG/Aa6XAateoFfwJFzjXwIXr4QXr7Idutc+RSkzQJ6waiHG/d3cRY08Z+h2NhYhg0bRs+ePbnsssu44oorjtp/6aWXMmXKFHr37k2XLl0YPHhwg1z3tdde45577qG0tJSOHTvyyiuvUFNTw0033URBQQHGGO677z6ioqL4wx/+wPz58/H19aV79+5cdtllDRKDUo7I3w3z/gzr34Ohv4CL/wqFmbDrW9vyL9gDL46xfes3fQj+7jGt0lzY9iXs+gYCI2yrfN070O9mO+gKdiD1uyng628/AfgH2wHXsFYw9Oe2K+ZE2g+Ecy+D/evh1pn2k0Lq7U3z+zgLYhrpBoGGlJqaao5diGXz5s1063aCvjJ12vR3qRxXmAlledCq+4n3r50BM39p+8qH/gJG/9Y9QHqMde/a6ZOte9vW9/p3YflL4Kq2nwqqy+3XkJ/bN47ad67vWgoxHSG81enFXl1pv9f+JOAhRGSlMSb12O3a4ldKOSv9W9uvXpYLSUNh1EPQ6fwj+3cvg09+bqdBXvMCRLar+1y9b7AzbT66B166AMTHtuwH3Apt+tlEX1kCgWHHv7bDkDOL3wMT/qnorB6llHPWvWfnrYfEwAV/sDNt3roe9q2x+wsy4J2bIao9THjz5En/kC6XwT3fwLBfwz3fwthnIHGA/YQgcuKk72UaLfGLyDQRyRKRDSfY94CIGBGJa6zrK6U83Pb5di580mA7h37kA/CThRAaDx/caW+AmnYpVJXBhOl28La+otrDRY/X3XXk5Rqzxf8qcOmxG0WkPXARsLsRr62U8mRZafDurRB3Lkx4+0hSD4mx3Tk52+1smZoquO1TSNDZaA2p0fr4jTGLRCT5BLv+AzwEfNJY11ZKNbHqyuP7usvyYdE/oXCfTezB0XZ2TMb3toZNcDT86J3j73pNGWlb69u+hPFT6te9o05Lkw7uishYYK8xZu2p6sCLyCRgEkBSUlITRKeUOiMlOfDcQGg3yLbWA8Jg0ycw51E7dTImxb4JlOWBqbFdOefdY6dQRtXxtz3sV/ZLNYomG9wVkRDgd8Af63O8MWaqMSbVGJMaH+8Zd7vVNnr0aL744oujtj311FMnrNZZ+zWHpqXWVZq5rhLMWppZeaylz9q58j98CVNHw7Op9kaqsHi4+2v4xUp4aDv8MQce2Q33p8ElTzR4xUlVf005q6cTkAKsFZF0oB2wSkROcoud55o4cSIzZsw4atuMGTOYOHFivV4/e/ZsoqKiGiEypRqQMZC91RYXqyw9fn9prq1V02M83DLTHhMcDde9AncvgLZ9jxwrYrt6fHUWudOaLPEbY9YbYxKMMcnGmGQgA+hvjNnfVDE0pOuuu45Zs2ZRUVEBQHp6Ovv27WP48OHce++9pKam0qNHDx577LETvv5QaWaAJ554gi5dunDhhReyZcuWU15bSzOrJpG1Gf7Tw3bjzPgRvHzx8YuCLHseKovtHbDJw+A3abaV3/MaTfAerNH+ZURkOjAaiBORDOAxY8zLjXKxzx+xt0w3pNa94LLJde6OjY1l0KBBzJkzh6uvvpoZM2Zw4403IiI88cQTxMTEUFNTw5gxY1i3bh29e/c+4XlWrlzJjBkzWL16NdXV1fTv358BAwacNDQtzawanavG3jRVVQZj/wt+wTD7N/DCKHtzVWgCHNwKu5ZA96uPTJvUNZybhUZr8RtjJhpj2hhj/I0x7Y5N+u6W/8HGun5TqN3dU7ub591336V///7069ePjRs3smnTpjrPsXjxYsaPH09ISAgRERGMHTv2pNfU0syqSSx/CfaugMv+Dv1vgd7Xw93zIWmILYe85i0oOQj9fgyX/t3paNVpahlZ4CQt88Y0btw47r//flatWkVZWRn9+/dn586dPPnkkyxfvpzo6Ghuu+22E5Zjru1UM5zqS0szqwaRu9MWQ+t8IfS6/sj22E7woxl1v041G1qy4SyEhYUxevRo7rjjjsOt/cLCQkJDQ4mMjOTAgQN8/vnnJz3HyJEj+eijjygrK6OoqIhPP/30pMfXLs0MnLA08z/+8Q/y8/MpLi5m+/bt9OrVi4cffpjU1FTS0tIa5odXLUfGSlg2xQ7UluXB2zeAjx9c8W/tummhWkaL30ETJ07kmmuuOdzl06dPH/r160ePHj3o2LEjw4adfFGI/v37c+ONN9K3b186dOjAiBEjTnlNLc2sGsTORfDV47ZLB2DhZIhsb1v8t3wM0R0cDU81Hi3LrPR32dLVVNmiZ/m7bIveN8BWvFz7tr2BasgvbBGzrx6D9MUwbgr0rd+0ZOXZtCyzUt4oeyt8cMfxs958/OyqUiMftIuOANz6qR2w9ZDlAVXj0cSv1NnI3QHfvQAX/Rn8Ak99fFNKm21XowoIsa34tn3tMoI1VTbZh8QcfbyIJn0v0awTvzGmwWbEeKvm0NXnEYr2w4ENdqZLbV8/ARveh7b9oM8EZ2I7keJs+OSnEH8u/Ojdk69Bq7xOs038QUFB5OTkEBsbq8n/DBljyMnJ0Zu6TqXoALxymW3dX/YPOO8ndnv+btj4kX38/YtNn/hn/BgylttVp/xDYe10+6lj3BT47nmoKIbxUzXpq+M028Tfrl07MjIyyM7OdjqUZi0oKIh27bTsbZ3K8uHNa2yLP3kEfP4QBMfYG5qWTbHdI0N+AUv+C/tW25Z/Q1v5mp1h03H0kW2Z6yBtFsR3s2UTXDW2nHHuDph2sV1jdtQjWsdenVCzTfz+/v6kpKQ4HYZq6WbdB9lb4Mfv2vVg37wWPrwLNn0MOxZAj2vsAOnyabDkWWjVw9aav/alo6dDFmba117xr9Nb2zUvHWb92vbN/2LVkdr1S56BgHC4Y45N+q4q27IvzYVPfwXFWTDi/ob7PagWRW/gUt7LVQMzfwkf3G0HPI+1cxFs/NAm9k4XgH+QfQMY/SjsXGyLkw39ha042edG29c/zz0vft6fjz7XylchayPMeQRcrvrHuOx5u2B4yUG7qAlA3i7Y8KFdQDw4CkJjj3TnhMTAjW/AnV943mCz8hjNtsWv1FkxBj5/GFa9Zp/7B8FVz9gEW54PUR3s/qgkGPbLI68LCIXRj9h+/rx0aOMuvjf8fqiphH4325WjFj8JQ34Gif2hphpWvW67iDLXwKaPoOe1p46xNNe+rtf14ONr3wTa9IF179oupsF1r/2g1Mlo4lfew1Vji4vtWwOFe2HrHNti9wuyrek9y+HgFjAu8A+BqlK48c0j89xrO7SU4CFR7eHq5+zjhO6w8hV7Q9QtM2HbXCjaBze8Dgv/AfP+Al2vrLtFboz9NPHdFBvDkJ/bVas2fmIXIRdfOwc/MrHBf0XKO2jiVy3T7mV2rdZD67VmroVPfw37VtmEHRAO590LF/7Ztp5Lc+0MmZEP2YS653vbjdL1ytO/dlCEPc+ch+HDSVCUCeFtoMvl9g3lrevgHx3t3bKB4bY1P+zX9tNBeSFMuwSy3BVdO42B1j3t41s/scscdhhiX6fUGdLEr5qX3B0QnVJ38bDqCpj7e7sqVEAYXPgnW3vmu+ftAOm1L9tulmNff+W/j37e/5azi3PgXVCaA9/8xw68jnwIfP3hnItg4jt2mcK9K+2atIX7IGMF3PONHRvIToPRv4XwVnDOJUfOmXjydRqUqq9mW6tHeZnyQpj9IKybAZf9E86bdPwxlaXw2lV2cHXQJLtQyI4Fdt+A2+HCx47unmkK2Vth9Rsw/L7j75Q9ZN9qeOkiOyMoc43t2rnkiSYNU7VMddXq0cSvPF/+bnj1SijIsF0mrmr41Vo7IFvbxz+FNW/DddPs0n/G2GmXkUnQzsNby0v+az+pRKfAvUtsmQWlzpIWaVPNU1U5vHuLrSp5xxzblfPalXY2zqE7aAFWvWEHbkc+ZJM+2O6cHuOdift0Df6ZHVTuNEaTvmp0jTaPX0SmiUiWiGyote2fIpImIutE5CMRiWqs66tmyBg7nbK6wj4uy4PPH7RdIeOnQPtBkDzc3kj1zX/smwLA5k/hs/shZZSdatkc+fjAsF8dGchVqhE1Zov/VeBZ4PVa274EHjXGVIvI34FHgYcbMQbVHJTmwvKXYf17djolgG8g1FTYx8Pvg65X2MciMPpheP1qeGEkdBxl14dNHADXv2pnyCilTqrREr8xZpGIJB+zbW6tp8uA6xrr+qoZMAY2fGDvZi3Jhg7D7OLdNZVQXgBhrSC289EzW8DWrLnmJVj2Pzt7p/OFdo58QKgjP4ZSzY2Tffx3AO/UtVNEJgGTAJKSkpoqJtWUvnoMvn3aFja76cMjd8HWR+/r7VfuTrtcoK8OVylVX47U6hGR3wHVwFt1HWOMmWqMSTXGpMbH6+IQLc43/7FJP/UOuGve6SX92mJSNOkrdZqa/C9GRG4FrgTGmOYwl1Q1nPRv7YLexdmQvdneSHX5v+zAplKqyTRp4heRS7GDuaOMMaVNeW3lsKL98O7Nti5O237Q7Uo79VKTvlJNrtESv4hMB0YDcSKSATyGncUTCHzpXjVrmTHmnsaKQTnIVQPbv7YVLFNG2WmZlaVw++cQ38Xp6JTyao05q2fiCTa/3FjXUx6iOBtWv27rz+fvPnrfVc9o0lfKA+iomDo9Lhd8dp+9yeriv0JonN1eU21n6Xz3gi1KljwCLnzc1o/f/rWdonm2hc+UUg1CE786teJs8AuwK03Nf8K25sXH1pkf+kto3cvWmtkxH/rdZLfVbtnHdnIsdKXU8TTxq5Mry4fnh0BFMXQ6H7bMtqtMDfkZzPyFbeUD+PjD2Geh/82OhquUOjVN/OrkFv7d1s/pdT2kzbJ1cq74t/0EcNdXdlHvAxvtgidx5zgdrVKqHjTxq7plb7ElEQbcClc9DVVl4ONnFxQ5JCzBfimlmg1N/OpoNVWwf51denD1W+AfChf8we470dqzSqlmRxO/smqq7ULeW+dAtbvccUQ7uOqpIzN3lFItgiZ+Za15065W1e9m6DwG2g2yi44rpVocTfzebPcySOhmZ+QsmGyT/dj/1r2QuVKqRdDE7602fAjv3267c5KHQVEmXPuyJn2lvIAmfm9UkAGzfm1vvKqugHXvwDkX2zcApVSLp4m/pcvfDfP/Bh2GQvdxkLsd5jxqi6jd8DqEt4HVb0KXy52OVCnVRDTxt2R5u+C1KyF/D6ydDjN/CRhbbmHcFIjpaI8bdLejYSqlmpYm/pbq4A/w5ni7du3dX9v5+Vtm28HcTmMgTFc1U8pbaeL3dGmfwa4lthLmqQZeq8ptaz59sR249fGDWz6xC58AJJ3X+PEqpTyeJn5PdnAbfHAXVJVCj2ug3YAj+/augm1fQkQb+3zNdNi95Mj+hB4wcTpEd2jamJVSHk8Tv6eqrrRJ3y8QEFg5zSb+wkyY8zBs+uTo42M6wYgH7NKG/kEw4HYIDHMkdKWUZ2vMpRenYRdVzzLG9HRviwHeAZKBdOAGY0xeY8XQrM17HDLXwI1vwbYvYP37cOGf4Z0fw4FNMOphOO8eqCi0JZNb9dA5+EqpemnMla5fBS49ZtsjwDxjzDnAPPdzdawNH8DSZ2Hg3XZR8gG32e6eVy6DvSth/BQ4/7cQEgPRydC6pyZ9pVS9Neaau4tEJPmYzVdjF2AHeA1YADzcWDF4lKL9kP4N5GyHyiI78HrupZA0+OjjDmyCT34O7QfDJX+z29r2h9a9bdXM1Dugx7gmD18p1XI0dR9/K2NMJoAxJlNE6izkLiKTgEkASUlJTRReIynLgynDoSQbENsPX1MBy6bA3fNsNw1AZQm8ewsEhsMNr9nFTsC25i963A7gHnozUEqpM+Sxg7vGmKnAVIDU1FTjcDhnZ/7foDQHbv4IkobYuvZF++GFkfDOzTBpvl3PdvZDkPMD3DoTwlsffY5OF9gvpZQ6S43Zx38iB0SkDYD7e1YTX79+ig7Ahz+xywoCGAP7VtvFSfLST+9c+9fD8pcg9U6buA8tZhLeGq5/1Z7vhVHw9o22NPKI30DKyAb8YZRS6mhNnfhnAre6H98KfHKSY52z6RNYNwNmP2CfL38Jpo6Gly+Cp/vAwn+c+hxl+bDqdXj/DgiOhgt+d/wxHYbCddMgthPs32DvqB39aEP+JEopdZzGnM45HTuQGyciGcBjwGTgXRG5E9gNXN9Y1z8rh26E2vSJrVO/+F/Q+UI4715Y8xbMfwKikqDPhBO/fv8GeH2s7d6J6Wjr4gRHn/jYHuN0sFYp1aQac1bPxDp2jWmsazYIY2yJhB7jbX/7gv+DiEQYPxVCY203TEm2nXnj6w89rz369Qc22qTvFwR3fgXtUnWqpVLKozR1V4/ny90BxQdsgr/6OVuz/rpXbNIHO9PmxjegbV/bjfPxT+0bxYGN8OUf4eVLwDcQbv0U2g/UpK+U8jgeO6vHMbuX2u9JQyGhK9zzzfHHBEfD7Z/Dwr/bbqA1b9nt4mNr3o/5I8SkNFnISil1OjTxH2vXEgiOgfguJz/O1x8u+D30v8UWUyvOsjdjacJXSnk4TfzH2rXEzrapbxdNVJL9UkqpZkL7+GsrzIS8nfYmK6WUaqE08de2ba793mGos3EopVQj0sR/iDHw3RRo1fPIilVKKdUCaeI/ZMd8yNoEQ36mUzCVUi2adyf+7V/DG9fAjoWw9DkIa3X8DVlKKdXCeO+snoPb4N1boaIIts+z2y74vXupQ6WUarm8M/GX5cH0CeAbAD9fAZs+gu3zbQVNpZRq4bwv8W+eBbMfhJIsuGUmxHWGkQ/aL6WU8gLe1ce/+k27WHlIDNwxF5KHOR2RUko1Oe9q8W+dY++ynbTAllxQSikv5F0t/r2rod1ATfpKKa/mPYm/OAsKM6Btf6cjUUopR3lP4t+7yn5P1MSvlPJu3pP4962y9fLb9HE6EqWUcpQjiV9E7hORjSKyQUSmi0hQo1907yqI7wYBoY1+KaWU8mRNnvhFJBH4JZBqjOkJ+AJ1rFreQIyxLf5ELb6mlFL1Svwi8isRiRDrZRFZJSIXn8V1/YBgEfEDQoB9Z3GuU8vfBaU5OrCrlFLUv8V/hzGmELgYiAduByafyQWNMXuBJ4HdQCZQYIyZe+xxIjJJRFaIyIrs7OwzudQROrCrlFKH1TfxH6pTfDnwijFmba1tp0VEooGrgRSgLRAqIjcde5wxZqoxJtUYkxofH38mlzpi0yfgFwQJPc7uPEop1QLUN/GvFJG52MT/hYiEA64zvOaFwE5jTLYxpgr4EGi8Ja82fwqbPobh94NfQKNdRimlmov6lmy4E+gL7DDGlIpIDLa750zsBgaLSAhQBowBVpzhuU7qlbkruGbZz4ls3RtG3N8Yl1BKqWanvi3+IcAWY0y+u1vm90DBmVzQGPMd8D6wCljvjmHqmZzrVIbueIrgmiKyL3r6qDINheVVLN52luMGSinVTNU38T8PlIpIH+AhYBfw+ple1BjzmDGmqzGmpzHmZmNMxZme62T8x/yWB6ru5cuDcUdtn/x5Gje//D37C8ob47JKKeXR6pv4q40xBjso+7Qx5mkgvPHCahgpnbuxMmIM87dkHd6WX1rJh6syAFienutUaEop5Zj6Jv4iEXkUuBn4TER8AY8vcSkinN81nm9/OEhFdQ0AM5bvobzKhb+vsEITv1LKC9U38d8IVGDn8+8HEoF/NlpUDeiCrgmUVtbw/c5cqmtcvL4knaGdYhmUEsPy9Dynw1NKqSZXr8TvTvZvAZEiciVQbow54z7+pjSkYxyBfj7MXp/JU19tY19BObcNTSa1Qwxp+wspLK9yOkSllGpS9ZrOKSI3YFv4C7A3bv1XRB40xrzfiLE1iOAAX4Z0imX693sA+wlgTLdWhAT44TKwenc+o849yxvElFKqGanvPP7fAQONMVkAIhIPfIWdlunx7hnViYTwQCYMSqJ/UjQAfZOi8PWx/fya+JVS3qS+id/nUNJ3y6EZ1fIf3DGWwR1jj9oWFuhH9zYROrNHKeV16pv454jIF8B09/MbgdmNE1LTGZgcw9vf76Ky2kWAX7N5H1NKqbNS38HdB7F31/YG+gBTjTEPN2ZgTaF/hyjKq1xsPVDkdChKKdVk6tvixxjzAfBBI8bS5HonRgGwLqOAnomRzgajlFJN5KSJX0SKAHOiXYAxxkQ0SlRNpH1MMFEh/qzfmw8kOR2OUko1iZMmfmOMx5dlOBsiQq/ESNZlnFG9OaWUapa8fkSzd7tItuwvoryqxulQlFKqSXh94u+VGEW1y5C2Xwd4lVLewesTf+92dlB3XUa+s4EopVQT8frE3yYyiLiwAO3nV0p5Da9P/IcGeNdr4ldKeQlHEr+IRInI+yKSJiKbRWSIE3Ec0qtdFNuyiiitrHYyDKWUahJOtfifBuYYY7pi7wTe7FAcAAzoEI3LwPc7tW6PUqrla/LELyIRwEjgZQBjTKUxJr+p46jtvJQYgvx9WLBFF2BXSrV8TrT4OwLZwCsislpEXhKRUAfiOCzI35dhneL4Oi0Lu7SwUkq1XE4kfj+gP/C8MaYfUAI8cuxBIjJJRFaIyIrs7MZviY/umsDu3FJ2Hixp9GsppZSTnEj8GUCGMeY79/P3sW8ERzHGTDXGpBpjUuPjG3+hlNHuxVjma3ePUqqFa/LE716/d4+IdHFvGgNsauo4jtU+JoRzEsJYsCXr1AcrpVQz5tSsnl8Ab4nIOqAv8DeH4jjK+V0T+G5HLiUVOq1TKdVyOZL4jTFr3N04vY0x44wxeU7EcazzuyRQWeNi0Vbt7lFKtVxef+dubQOTo4kO8eeLjfudDkUppRqNJv5a/Hx9uLBbK+alZVFZ7XI6HKWUahSa+I9xSY/WFJVXs2xHjtOhKKVUo9DEf4zh58QREuCr3T1KqRZLE/8xgvx9Gd0lni83HcDl0rt4lVItjyb+E7ikR2uyiiq0u0cp1SJp4j+BS3q0Ji4sgKmLdzgdilJKNThN/CcQ5O/LbUOTWbAlm82ZhU6Ho5RSDUoTfx1uGtyBkABfXli43elQlFKqQWnir0NUSAATByXx6bpMMvJKnQ5HKaUajCb+k7hzeAoCvLR4p9OhKKVUg9HEfxJto4IZ27ct7yzfQ15JpdPhKKVUg9DEfwo/GdmJsqoaXl+6y+lQlFKqQWjiP4UurcO5oGsCry7ZyfL0XLKKyp0OSSmlzoom/nr42fmdKCqv5vopSxn0xDz+8PEGLeKmlGq2/JwOoDkY0CGGBQ+O5oesYhZsyebVJelszixk6i2pxIQGOB2eUkqdFm3x11O76BBGd0ngT2N78N+J/ViXUcBfZzm+YqRSSp02Tfxn4Ko+bbl7ZAofrt7Lyl25ToejlFKnxbHELyK+IrJaRGY5FcPZ+OnozrSOCOJPMzdRo1U8lVLNiJMt/l8Bmx28/lkJDfTj0cu7sn5vAVMXaTE3pVTz4UjiF5F2wBXAS05cv6GM7dOWK3q34e9z0vhkzV6nw1FKqXpxalbPU8BDQHhdB4jIJGASQFJSUtNEdZpEhH9d34ec4goeeG8tczcdID4skDuGpZAUG+J0eEopdUJN3uIXkSuBLGPMypMdZ4yZaoxJNcakxsfHN1F0py/I35ept6QypmsrNu0r5O3vdvPQB2udDkspperkRIt/GDBWRC4HgoAIEXnTGHOTA7E0iIggf6bcPACAad/s5M+zNrE8PZeByTEOR6aUUsdr8ha/MeZRY0w7Y0wyMAH4ujkn/WNNHJREbGgA//36B6dDUUqpE9J5/A0sOMCXu0Z0ZNHWbNbsyXc6HKWUOo6jid8Ys8AYc6WTMTSGm4d0IDrEn19MX8WeXF3ERSnlWbTF3wjCAv149fZBFJRWceMLS5n8eRo3v/wdr36rC7oopZynib+R9GkfxfRJg6mscfHS4h1sO1DM47M28e0PB4871uUy7MopYV9+mQORKqW8jRjj+eUGUlNTzYoVK5wO44xUVrtwGYPLGK5+9lvySiv57JcjaBURxJ7cUiZ/nsb8LVmUVtYA0D8pihtS23PtgHb4++r7slLqzInISmNM6nHbNfE3nW0Hihj77LdUu1x0aR3O1gPF+Ipw7YBEeiVGcrC4kplr9rHlQBHJsSFMGtmJ1ORoOsWH4esjToevlGpmNPF7iPUZBXy2PpN1GfkkRgVz/8Xn0iYy+PB+Ywxfp2Xxjzlb2HKgCIDWEUH8ZVxPLureyqmwlVLNkCb+ZsblMuw4WMzaPQW8uHgHafuLuKJ3G353eTfaRgWf+gRKKa9XV+LXFbg8lI+P0DkhnM4J4VzVpy1TFm7n2fk/MG/zAe4cnsJVfdrSpVU4ItoFpJQ6Pdrib0YODQZ/tj4TgI5xofx1fE+GdopzODKllCfSrp4W5EBhOV+nZTF10Q52HixhfL9EwoP8qKhykRQbQvc2EYw6Nx4fHRBWyqtpV08L0ioiiImDkhjXN5G/z0lj+ve7CQ7wxc/Hh4PFFYC9j+CJcT3pmRjpcLRKKU+jLf4Wpqi8ii83HeBvs9M4WFzBoOQYrurblusHtCPI39fp8JRSTUi7erxMYXkVry9J55M1+9iWVUy76GB+d3k3BqXEEBHsrzeHKeUFNPF7KWMMS3fk8KeZG9l6oBgAH4GByTFc2bsNV/VpS1RIgMNRKqUagyZ+L1dV42Le5iwOFJaTWVDOl5v2sz27hCB/H8b3S+TiHq3pnxRNZLC/06Eq7H0c324/iMtAl1bhtIoI1Km76rRp4ldHMcawcV8hbyzdxcdr9lJR7UIEzu+SwL2jO+nqYU2grLKGIH8fRISi8io+WbOPnOJKql0uPl27j/ScIyW9x/Zpy1M39tWZWuq0aOJXdSqtrGbNnnyW/JDD29/vJrekkuGd4/jDld3p0jrc6fBanJW78nj5mx18sfEA0SEB9E+KYun2HIoqqg8f07d9FHcMTyEhPJB5mw/w4uKd3DU8hd9e3o21GfkkRASRqHdwq1PQxK/qpayyhre/380z87ZRXFHNFb3acOPA9gzpGKutzQawclce101ZQkSQP+P7JZJbUsnKXXn0bR/FT0Z1pEfbSIwx+NUafDfG8Pinm3h1STqxoQHklFSSEhfK3PtG6iC9Oimdx6/qJTjAlzuHp3BNv0Senf8D763Yw8y1+wgL9KNXYiSX9mzNhEHtCfTTqaGnyxjDE59tIi4skHm/GUVEUF3jKUe/wYoIf7iyOxXVNeSVVJEcF8qUhdt5f2UGEwclNX7gqsVp8ha/iLQHXgdaAy5gqjHm6ZO9Rlv8zimvquGrzQf4bkcuK3blsTmzkMSoYMb3S6RNVBDto0Po0jqchHAdfDyVz9dncu9bq5h8TS8mnEXCNsZwzfNLyMwvZ8GDo/X+DFUnj+nqEZE2QBtjzCoRCQdWAuOMMZvqeo0mfs9gjOGbHw7yr7lbWZeRj6vWf50urcJ5/OoeDO4Y61yAHqyiuoaL/7OIID9fZv9qxFmvr7B0ew4TX1zGz87vxP0XddH1GtQJeUxXjzEmE8h0Py4Skc1AIlBn4leeQUQYcU48I86Jp7rGRXZxBTsPlpCWWcS0b3cyYeoy+raPIjzI/rcqqaimdWQQj1zajaTYEIejd9bkz9PYlVPK63cMapAkPaRTLBd3b8Vz87cze/1+7rvoXMb2adsAkSpv4OjgrogkA4uAnsaYwmP2TQImASQlJQ3YtWtX0weo6q2ssoYXFm1nyfYcKqtdGCAs0Je1ewqodrmYMDAJYwwhgX7cPjSZhIggp0NuMnM37mfSGyu5fVgyj13Vo8HOW+MyfLFxP8/N/4GN+wqZOCiJx67qrl0/6jCP6eo5fGGRMGAh8IQx5sOTHatdPc1XZkEZf/xkI1+nZREW6EdJRTV+vsLtw1K4fVgyCeFBlFRUs/NgCV1ah7e4WSrZRRVc+O+FtI8J5oN7hzbKoHh1jYt/fbmV5xdsp3ubCP5+bW96tdPifMrDEr+I+AOzgC+MMf8+1fGa+Js/Ywwiwu6cUv715RZmrt2Hv48PfdtHsSYjn8pqF+GBfpzXMYbwIH9CA32ZMDCp2VcX/cecNJ5fuJ0v7xtF54SwRr3Wl5sO8NuP1pNTXMG9ozvxwMVddMDdy3lM4hf7P/E1INcY8+v6vEYTf8uz82AJ077ZyfL0XIZ0iqV3u8jDM4cqq13kFFdQUlnDJT1acd9F59K1dYTTIZ+24opqhv7fPIZ1juP5mwY0yTULyqr4y6xNvL8yg3tHd+LhS7s2yXWVZ/KYwV1gGHAzsF5E1ri3/dYYM9uBWJRDUuJC+cu4nkdtG9+v3eHHheVVTPtmJy8v3sncTYu5olcbLu7Rmh5tIzhYVMGu3FJS4kLp3S7SY+8peGf5HgrLq5k0smOTXTMy2J9/XtebAD8fnl+wnchgf+4Z1anJrq+aBydm9XzDsXeoKHWMiCB/fn3hudw2NJmpi3bw2pJ0Zq3LPO64AD8fOsWH0Sk+lCt7t+Hi7q2PusPY5TKO3HFcXeNi2jc7GZgcTb+k6Ca9tojwl6t7UlRezeTP08gtqeSRS7vqndfqML1zV3m0qJAAHrq0K/dddC5b9hexObOQhIgg2kcH80NWMSt25bHtQBEr0vOYtS6Trq3DSYoJOVyF9GBxBZ0TwrhpcAeu7pNIZEjTVB/9eM0+9uaX8aexDTeL53T4+gj/uaEP0SH+TF20g/SDJfz92t5Eh2oJbqW1elQLUeMyfLp2Hy8u3kFVjYtWEUG0iQwiLiyQb344yLqMAnwE+iVFc15KDD0TI+nZNpL2McENPgBaWe3ign8tICrEn09/PtzRAVZjDNO+TWfy55uJDPbnD1d25/JebVrc7Cl1Yh4zuHsmNPGrs7U+o4AvN+1n4dZsNu4rpNp923FEkB/DOsdx98iO9G+gLpk3l+3i9x9v4JXbB3J+l4QGOefZ2pxZyAPvrWXjvkKiQ/y5um8iPz2/Ewnh3nM/hTfSxK+UW3lVDVsPFLFxXyHrMgqYvT6TgrIqurWJYEjHWLq3jSA6xJ/48EA6xofhK8Lq3XkUllcxukvCSW+Q2l9QztXPfUP76BDeu2eIR02nrK5x8XVaFjPX7uOLjfsJ8PXhp+d35s7hKXrTVwuliV+pOpRUVPPO8j3M3bSf1bvzqah2HbXf10eocX9CiA0NYGzftrSLDiExKoihneMI9vflkzX7eGNpOmszCvD1EabfPZhBKZ67mM3OgyX83+zNzN10gMSoYB645FyGd44nPjzQ6dBUA9LEr1Q9VFa72JdfRkFZFZkF5WzPLqassoYBydH4ivD60l0s3JpFVY39u/HzEaJDA8guqqBr63Cu6tOWS3u2plN8496s1VCWbD/IX2ZtZnOmrZjSNjKI24Ylc9PgDoQE6NyP5k4Tv1INxBhDYVk127KK+HLzAXZkl3BjanvGdEvwqK6d+qpxGZan57JhbwHzt2Tx7Q85RIf4M+KceM7rGMMlPVoTF6afBJojTfxKqXpZkZ7La0t38d2OHLKKKvD1EYZ3jqNVRCD+vj50bR3OgA4xtIsJJjzQr1m+2XkLT7pzVynlwVKTY0hNjsEYw9YDxXy0ei9zN+1n64EiyqpqeOu73YePDfDz4aJurZg0siNd24RjDLiMwWUgxN9XbxrzUNriV0rVmzGGjLwyVu3OI6uwgj15pXy0ei9F5dXHHRvg50O7qGDaxYTQLjqYXomRjOma4FUluZ2mXT1KqUZRVF7FzLX7yC+tQgR83F0/uSWVZOSVkpFXxu7cUvJLqwBbp6ljXCg9EyMZ3SWelLhQcksqCQvy0/sKGpgmfqWUY4wxbDlQxLzNWWzcV8D2rBK2ZhVxbPrp2z6KfklRGGMHnatdhtAAX/p3iCa1Q7R+WjhNmviVUh4lt6SSxduyyS6qIDYsgL15ZXyx8QA7sovx9RH8fH3w9REKy6oO31vRPiaYQcmxjO4Sz5BOsQT5++LnIwT5+2KMIbu4gsz8cs5tFU5wgN6UpolfKdUsVVa72JRZyIr0XFbuymPZjhzy3N1GhwT6+RDg53N4rMHPR+iZGEnX1uF0ig8jISKQ2NBAeiZGEBXiPYXqNPErpVqEGpdhzZ481uwpwOUyVNa4KCiroryqhpS4UFpHBLF+bwErduXxQ1YxuSWVR73+3FZhdE4IIzEqmKoaQ2F5FRFB/rSJDKJ72wj6J0UTGtgyJjxq4ldKeaWC0iqyiys4UFjO6t15rNyVx66cUvbmlxHg60N4kB+F5dUUV9hPC74+QuuIIKJD/Qn088VHICTAj5jQAKpdhrySSoL8fTm3VRgJ4YGICFEh/pzbKpz2MSEE+vng5yMecX+DzuNXSnmlyBB/IkP86ZwQxrDOcXUeV1BWxdo9+axIz2Vvfjl5pZVUVruocRnySyvZnl2Mn48QFRLA/sJyFmzJOlzl9VgBfj7EhwUSFx5IfFggMaH+BPn7Eh7kR8e4MFLiQ4kLDSQy2J9ql4vSyhqyiyvIKa6kxmXw8xF6tYukVSMNZjuS+EXkUuBpwBd4yRgz2Yk4lFLqkMhgf0aeG8/Ic+PrdXxltYviimqMMRwsriRtfyH7C8oPb88uriC7qIKMvFI27K2ivLqGovLqwwX/6iMpJoTJ1/ZiaKe637DORJMnfhHxBZ4DLgIygOUiMtMYs6mpY1FKqTMV4OdDjJ8dKI4NC6RL6/BTvqaqxsWunFJ25ZSQU1JJUXk1/r5CoJ8PCeFBxIYF4OfjQ1lVDat357E8PbdRWv1OtPgHAT8YY3YAiMgM4GpAE79SqkXz9/Whc4IdXD6VAR2iuWtEx0aJw4n11xKBPbWeZ7i3HUVEJonIChFZkZ2d3WTBKaVUS+dE4j/RUPdxnV7GmKnGmFRjTGp8fP363JRSSp2aE4k/A2hf63k7YJ8DcSillFdyIvEvB84RkRQRCQAmADMdiEMppbxSkw/uGmOqReTnwBfY6ZzTjDEbmzoOpZTyVo7M4zfGzAZmO3FtpZTydk509SillHKQJn6llPIyzaJIm4hkA7vO8OVxwMEGDKcxaIwNQ2M8e54eH2iMp6ODMea4+fDNIvGfDRFZcaLqdJ5EY2wYGuPZ8/T4QGNsCNrVo5RSXkYTv1JKeRlvSPxTnQ6gHjTGhqExnj1Pjw80xrPW4vv4lVJKHc0bWvxKKaVq0cSvlFJepkUnfhG5VES2iMgPIvKIB8TTXkTmi8hmEdkoIr9yb48RkS9FZJv7e7QHxOorIqtFZJYnxigiUSLyvoikuX+fQzwwxvvc/84bRGS6iAQ5HaOITBORLBHZUGtbnTGJyKPuv58tInKJgzH+0/1vvU5EPhKRKE+Lsda+B0TEiEhcrW1NHuPJtNjEX2uJx8uA7sBEEenubFRUA78xxnQDBgM/c8f0CDDPGHMOMM/93Gm/AjbXeu5pMT4NzDHGdAX6YGP1mBhFJBH4JZBqjOmJLUg4wQNifBW49JhtJ4zJ/X9zAtDD/Zr/uf+unIjxS6CnMaY3sBV41ANjRETaY5eV3V1rm1Mx1qnFJn5qLfFojKkEDi3x6BhjTKYxZpX7cRE2WSW643rNfdhrwDhHAnQTkXbAFcBLtTZ7TIwiEgGMBF4GMMZUGmPy8aAY3fyAYBHxA0Kw6044GqMxZhGQe8zmumK6GphhjKkwxuwEfsD+XTV5jMaYucaYavfTZdh1PDwqRrf/AA9x9OJSjsR4Mi058ddriUeniEgy0A/4DmhljMkE++YAJDgYGsBT2P+8rlrbPCnGjkA28Iq7O+olEQn1pBiNMXuBJ7Etv0ygwBgz15NirKWumDz1b+gO4HP3Y4+JUUTGAnuNMWuP2eUxMR7SkhN/vZZ4dIKIhAEfAL82xhQ6HU9tInIlkGWMWel0LCfhB/QHnjfG9ANKcL7r6SjufvKrgRSgLRAqIjc5G9Vp87i/IRH5HbbL9K1Dm05wWJPHKCIhwO+AP55o9wm2Ofp7bMmJ3yOXeBQRf2zSf8sY86F78wERaePe3wbIcio+YBgwVkTSsd1jF4jIm3hWjBlAhjHmO/fz97FvBJ4U44XATmNMtjGmCvgQGOphMR5SV0we9TckIrcCVwI/NkduQPKUGDth3+TXuv922gGrRKQ1nhPjYS058XvcEo8iIth+6c3GmH/X2jUTuNX9+Fbgk6aO7RBjzKPGmHbGmGTs7+xrY8xNeFaM+4E9ItLFvWkMsAkPihHbxTNYRELc/+5jsGM6nhTjIXXFNBOYICKBIpICnAN870B8iMilwMPAWGNMaa1dHhGjMWa9MSbBGJPs/tvJAPq7/696RIxHMca02C/gcuwMgO3A7zwgnuHYj3jrgDXur8uBWOxsim3u7zFOx+qOdzQwy/3Yo2IE+gIr3L/Lj4FoD4zxcSAN2AC8AQQ6HSMwHTvmUIVNTneeLCZs98V2YAtwmYMx/oDtJz/0dzPF02I8Zn86EOdkjCf70pINSinlZVpyV49SSqkT0MSvlFJeRhO/Ukp5GU38SinlZTTxK6WUl9HEr1QjEJHRhyqbKuVpNPErpZSX0cSvvJqI3CQi34vIGhF5wb0OQbGI/EtEVonIPBGJdx/bV0SW1aoJH+3e3llEvhKRte7XdHKfPkyOrBnwlvsOXkRksohscp/nSYd+dOXFNPErryUi3YAbgWHGmL5ADfBjIBRYZYzpDywEHnO/5HXgYWNrwq+vtf0t4DljTB9sPZ5M9/Z+wK+x60F0BIaJSAwwHujhPs9fG/NnVOpENPErbzYGGAAsF5E17ucdseWo33Ef8yYwXEQigShjzEL39teAkSISDiQaYz4CMMaUmyO1ZL43xmQYY1zYMgPJQCFQDrwkItcAtevOKNUkNPErbybAa8aYvu6vLsaYP53guJPVNTlRyd1DKmo9rgH8jF1MZBC2Qus4YM7phazU2dPEr7zZPOA6EUmAw2vPdsD+XVznPuZHwDfGmAIgT0RGuLffDCw0dj2FDBEZ5z5HoLs2+wm512KINMbMxnYD9W3wn0qpU/BzOgClnGKM2SQivwfmiogPttLiz7ALu/QQkZVAAXYcAGzJ4inuxL4DuN29/WbgBRH5s/sc15/ksuHAJyIShP20cF8D/1hKnZJW51TqGCJSbIwJczoOpRqLdvUopZSX0Ra/Ukp5GW3xK6WUl9HEr5RSXkYTv1JKeRlN/Eop5WU08SullJf5f0ZjNNGP4P2bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(eval_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train loss','Valid loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train fasttext model\n",
    "obtain weight from fasttest\n",
    "then embedding can load weights\n",
    "\n",
    "- train 2 fasttext model, 1 for text and another for abstract\n",
    "- obtain weights for both vocab\n",
    "- put in encoder and decoder like normal\n",
    "- https://stackoverflow.com/questions/31440803/how-to-fetch-vectors-for-a-word-list-with-word2vec\n",
    "- use this to update the vocab class to update ways to fetch words\n",
    "- to solve querying, use the model and use function to find cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_unsupervised('data/fil9')\n",
    "model.save_model(\"result/fil9.bin\")\n",
    "model = fasttext.load_model(\"result/fil9.bin\")\n",
    "# EXPECTS TEXT FILE FORMAT WHERE 1 TEXT PER ROW\n",
    "# IMPORTANT, NEED TO REDESIGN CLEANING PROCESS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring, all highlight:article ratio being more than 1.0 means the summary text is producing more text than the original text, this defeats the purpose of the model, therefore all data with a ratio of more than or equal to 1.0 will be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model improvement/Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Finalisation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "best_model_path = \"best_decoder20230719.pth\"\n",
    "best_model = Model(len(text_vocab),len(abstract_vocab),2,256)\n",
    "best_model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLOUR = \"<font color='Black'>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file):  \n",
    "    file_content = list(file)[0]\n",
    "    content = file_content[\"content\"]\n",
    "    return fitz.open(stream=content, filetype=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a19ebb6eb444c7a48e69f6b72a1e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.pdf', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploader = widgets.FileUpload(multiple=False, accept='.pdf')\n",
    "def on_upload(change):\n",
    "    #ch = change.value.values()\n",
    "    # to obtain information regarding the pdf when the pdf is uploaded, buttonless design, will impact perf\n",
    "    global doc_len\n",
    "    doc_len = len(load_pdf(uploader.value.values()))-1\n",
    "    \n",
    "    uploader.value.clear()\n",
    "    uploader._counter=1\n",
    "\n",
    "uploader.observe(on_upload, 'value')\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Generation tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radio button for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc486a5a33644c398acbe99106b604f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description=\"<font color='Black'>Model:\", options=('Base', '512Dim', '1024Dim', 'Base+No Stop wor…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "radio = widgets.RadioButtons(\n",
    "    options=['Base', '512Dim', '1024Dim', 'Base+No Stop word'],\n",
    "#    value='pineapple', # Defaults to 'pineapple'\n",
    "#    layout={'width': 'max-content'}, # If the items' names are long\n",
    "    description=TEXT_COLOUR+'Model:',\n",
    "    disabled=False\n",
    ")\n",
    "display(radio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 input box for model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9777934801604db1a1b03dd2b47e223c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=100, description=\"<font color='Black'>Max length:\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Max Length\n",
    "length = widgets.IntText(\n",
    "    value=100,\n",
    "    description=TEXT_COLOUR+'Max length:',\n",
    "    disabled=False\n",
    ")\n",
    "display(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835d6aed81b540248a603967682a6993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedFloatText(value=0.5, description=\"<font color='Black'>Temperature:\", max=1.0, step=0.1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature\n",
    "temperature = widgets.BoundedFloatText(\n",
    "    value=0.5,\n",
    "    min=0,\n",
    "    max=1.0,\n",
    "    step=0.1,\n",
    "    description=TEXT_COLOUR+'Temperature:',\n",
    "    disabled=False\n",
    ")\n",
    "display(temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ea0a3b39ee4b9c8b111477ef056651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntRangeSlider(value=(0, 21), continuous_update=False, description=\"<font color='Black'>Page Range:\", max=21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PDF start and end\n",
    "pdf_range = widgets.IntRangeSlider(\n",
    "    value=[0, doc_len],\n",
    "    min=0,\n",
    "    max=doc_len,\n",
    "    step=1,\n",
    "\n",
    "    description=TEXT_COLOUR + \"Page Range:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    ")\n",
    "display(pdf_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation button print result to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656df4bd6bac4ec3b865b567baf8d9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Click me', icon='check', style=ButtonStyle(), tooltip='Click me')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df12ba71a2c1402487780c496ddb7b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button = widgets.Button(\n",
    "    description='Click me',\n",
    "    disabled=False,\n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "#button.style.button_color=\"green\"\n",
    "global i\n",
    "i = 0\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "display(button, out)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with out:\n",
    "        global i\n",
    "        out.clear_output()\n",
    "        print(i,\" Button clicked.\")\n",
    "        i +=1\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap in grid widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ae8cd0a14f487fb570d48fef221599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(GridBox(children=(RadioButtons(description=\"<font color='Black'>Model:\", options=('Base', '512Di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = [radio, length, pdf_range, temperature, button]\n",
    "grid = widgets.GridBox(widget, layout=widgets.Layout(grid_template_columns=\"repeat(1, 100px)\"))\n",
    "grid_list = [grid]\n",
    "tab = widgets.Tab()\n",
    "tab.children=grid_list\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Query tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text input box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4fbbc8bbd947fe956c5799fccb436f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description=\"<font color='Black'>Query:\", placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type something',\n",
    "    description=TEXT_COLOUR+'Query:',\n",
    "    disabled=False   \n",
    ")\n",
    "display(text_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input box for start and end page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3be87298fb4687811937a6ff282b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntRangeSlider(value=(0, 21), continuous_update=False, description=\"<font color='Black'>Page Range:\", max=21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf_range_q = widgets.IntRangeSlider(\n",
    "    value=[0, doc_len],\n",
    "    min=0,\n",
    "    max=doc_len,\n",
    "    step=1,\n",
    "\n",
    "    description=TEXT_COLOUR + \"Page Range:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    ")\n",
    "display(pdf_range_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation button print result to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bd82f56f394083943c16c42da1c0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Click me', icon='check', style=ButtonStyle(), tooltip='Click me')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e792cb9f0149a29a6a694138e17dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button_q = widgets.Button(\n",
    "    description='Click me',\n",
    "    disabled=False,\n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "#button.style.button_color=\"green\"\n",
    "global j\n",
    "j = 0\n",
    "out_q = widgets.Output(layout={'border': '1px solid black'})\n",
    "display(button_q, out_q)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with out_q:\n",
    "        global j\n",
    "        out_q.clear_output()\n",
    "        print(j,\" Button clicked.\")\n",
    "        j +=1\n",
    "\n",
    "button_q.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap in grid widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d449c2ba2aad4198a7ddb177b4f9fbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(GridBox(children=(Text(value='', description=\"<font color='Black'>Query:\", placeholder='Type som…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget_q = [text_box, pdf_range_q, button_q]\n",
    "grid_q = widgets.GridBox(widget_q, layout=widgets.Layout(grid_template_columns=\"repeat(1, 100px)\"))\n",
    "grid_list_q = [grid_q]\n",
    "tab = widgets.Tab()\n",
    "tab.children=grid_list_q\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final combined tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b4a6202fa9491797cf9febb354cf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(GridBox(children=(RadioButtons(description=\"<font color='Black'>Model:\", options=('Base', '512Di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grids = [grid, grid_q]\n",
    "tabs = widgets.Tab()\n",
    "tabs.children=grids\n",
    "tabs_titles = [\"Abstract Generation\", \"Relevance Query\"]\n",
    "for i, title in enumerate(tabs_titles):\n",
    "    tabs.set_title(i, title)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html\n",
    "w = widgets.IntSlider()\n",
    "out = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dffecdeadc4ede95cfe5b406cd1e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecd71bff24f4338b76e0c4c2d99ed1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(out)\n",
    "display(w)\n",
    "#out.clear_output()\n",
    "with out:\n",
    "    out.clear_output()\n",
    "\n",
    "    for i in range(5):\n",
    "        print(i, 'Hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text(value='0', description='Abstract Generation'), Text(value='0', description='Relevance Query')]\n",
      "[Text(value='1', description='Abstract Generation'), Text(value='1', description='Relevance Query')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334690b79aea4f8c8b2ec9e1b68c351a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(GridBox(children=(Label(value='0'), Label(value='1'), Label(value='2'), Label(value='3'), Label(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_titles = [\"Abstract Generation\", \"Relevance Query\"]\n",
    "tab = widgets.Tab()\n",
    "items = [widgets.Label(str(i)) for i in range(8)]\n",
    "grid = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(3, 100px)\"))\n",
    "gridlist= [grid]\n",
    "for i, title in enumerate(tab_titles):\n",
    "    tab.set_title(i, title)\n",
    "    children = [widgets.Text(value=str(i),description=name) for name in tab_titles]\n",
    "    print(children)\n",
    "    tab.children=gridlist\n",
    "tab\n",
    "\n",
    "# work flow is as follows\n",
    "# widget1 = widgets.button()\n",
    "# widget2 = widgets.button()\n",
    "# widget3 = widgets.RadioButton()\n",
    "# widgetList = [widget1,widget2,widget3]\n",
    "# grid1 = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(3, 100px)\"))\n",
    "# grid2 = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(3, 100px)\"))\n",
    "# grids = [grid1, grid2]\n",
    "\n",
    "# tab.children = grids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPDF(file, start_page=0, end_page=-1):\n",
    "    '''\n",
    "    Will accept input of widget pdf file\n",
    "    start_page indicates starting page to start reading default=first page\n",
    "    end_page indicate last page to read default=last page\n",
    "    '''\n",
    "    doc = load_pdf(file)\n",
    "    if end_page == -1:\n",
    "        end_page = len(doc)-1\n",
    "    \n",
    "    # extracting text from page\n",
    "    doc_text = \"\"\n",
    "    for page in doc.pages(start_page, end_page):\n",
    "        \n",
    "        text = page.get_text(\"text\")\n",
    "        text = text.split('\\n')\n",
    "        text = \" \".join(text)\n",
    "        doc_text += text \n",
    "    doc_text = doc_text.strip()\n",
    "    doc.close()\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process(raw_data, stop_word=True):\n",
    "    data = cleanLine(raw_data, stop_word)\n",
    "    data_id = tokenise(text_vocab, data)\n",
    "    return data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Infer(model, data, max_size=100, temperature=0.5):\n",
    "    decoder_outputs = model(data, max_len=max_size, temperature=temperature)\n",
    "    decoder_outputs = torch.unbind(decoder_outputs, 1)\n",
    "    sentence = \"\"\n",
    "    for output in decoder_outputs:\n",
    "        token = output.argmax(1)\n",
    "        # if word is pad then replace with space\n",
    "        # if word is end then stop\n",
    "        if token == 2:\n",
    "            sentence += \" \"\n",
    "        elif token == 1:\n",
    "            break\n",
    "        else:\n",
    "            sentence += abstract_vocab.get_word(token.item()) + \" \"\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = readPDF(uploader.value.values(), start_page=0, end_page=-1) # Obtain 1 long string\n",
    "data = Process(raw_data) # Clean string and convert to word_ids\n",
    "output = Infer(best_model, data, max_size=100, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
